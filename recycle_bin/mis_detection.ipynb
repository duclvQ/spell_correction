{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "pipeline = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"vinai/phobert-base\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "def remove_special_characters(text):\n",
    "    import re\n",
    "    # Remove special characters and digits, keep only Vietnamese characters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z√Ä-·ªπ\\s]', '', text)\n",
    "    return text.strip()\n",
    "def topk_predictions(text, top_k=5):\n",
    "    # input_ids = tokenizer([text], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # logits = bartpho(input_ids).logits\n",
    "    # masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "    # probs = logits[0, masked_index].softmax(dim=0)\n",
    "    # values, predictions = probs.topk(top_k)\n",
    "    \n",
    "    # return tokenizer.decode(predictions).split()\n",
    "    \n",
    "    output = pipeline(text, top_k=top_k)\n",
    "    mask_index = text.index('<mask>')\n",
    "    # Extract the predictions from the output\n",
    "    predictions = []\n",
    "    for item in output:\n",
    "        if 'token_str' in item:\n",
    "            predictions.append(item['token_str'])\n",
    "    \n",
    "    predictions = [remove_special_characters(item['token_str']) for item in output]\n",
    "\n",
    "def sentence_prediction(text, top_k=300):\n",
    "    error_indices = []\n",
    "    for i in range(len(text.split())):\n",
    "        original_word\n",
    "        new_list = text.split()\n",
    "        # replace the i-th word with a mask token\n",
    "        # clean_word = remove_special_characters(new_list[i])\n",
    "        clean_word = new_list[i]\n",
    "        new_list[i] = \"<mask>\"\n",
    "        masked_text = \" \".join(new_list)\n",
    "        start_time = time.time()\n",
    "        predictions = topk_predictions(masked_text, top_k=top_k)\n",
    "        # for j in range(len(predictions)):\n",
    "        #     predictions[j] = remove_special_characters(predictions[j])\n",
    "        end_time = time.time()\n",
    "        if clean_word not in predictions:\n",
    "            error_indices.append((i, clean_word, predictions[0]))\n",
    "            print(f\"Masked Text: {masked_text}\")\n",
    "            print(f\"->Top {top_k} Predictions: {predictions}\")\n",
    "            \n",
    "        # print(f\"Masked Text: {masked_text}\")\n",
    "    return error_indices\n",
    "\n",
    "def paragraph_prediction(text, top_k=300):\n",
    "    sentences = text.split('. ')\n",
    "    all_errors = []\n",
    "    for sentence in sentences:\n",
    "        errors = sentence_prediction(sentence, top_k=top_k)\n",
    "        if errors:\n",
    "            all_errors.append((sentence, errors))\n",
    "            \n",
    "    return all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1dc03db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh <mask> Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng 20/8 t·ªõi ƒë√¢y.\n",
      "->Top 100 Predictions: ['Th', 'T', 'C', 'H√πng', 'Ph', 'Ho', 'Trung', 'Ph·ª•', 'Kh', 'Qu', 'Tu·∫•n', 'L', 'ƒê·ª©c', 'Vinh', 'Kh√°nh', 'Tr', 'Thu·∫≠n', 'H·ªìng', 'H', 'Ti·∫øn', 'Vi·ªát', 'S∆°n', 'H√†', 'Giao', 'Th·∫Øng', 'Thu', 'Ngh', 'Khoa', 'Hi·ªáp', 'Kh·∫£', 'B√¨nh', 'Di·ªÖn', 'Tu', 'Ki', 'Ch√¢u', 'Qu√Ω', 'S', 'T√†i', 'Hu', 'Phi', 'B√°', 'D', 'Phong', 'Vi√™n', 'ƒê', 'Ng', 'T√¢n', 'T√¢m', 'H·ª£p', 'Ng·ªçc', 'Ch', 'Ph√∫c', 'Th√°i', '√Å', 'H∆∞ng', 'Ph√∫', 'Thanh', 'D≈©ng', '√Ç', 'Duy', 'To√†n', 'Quy', 'Li√™n', 'Dung', 'Ph∆∞∆°ng', 'Long', 'Th·∫£o', 'Nguy√™n', 'Ch∆∞∆°ng', 'Ho√†n', 'X', 'Tr·ªçng', 'Th·ªã', 'M', 'B√≠', 'Thi', 'Di', 'Hi', 'Th√†nh', 'ƒê·ªô', 'Minh', 'Tho', 'Loan', 'VƒÉn', 'Tru', 'Quang', 'Hi·ªÉn', 'V', 'Th√¢n', 'L·ª£i', 'B·∫£o', 'B', 'Tuy', 'Anh', 'Tha', 'K', 'San', 'H·∫£i', 'Ch√≠nh']\n",
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu <mask> nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng 20/8 t·ªõi ƒë√¢y.\n",
      "->Top 100 Predictions: ['tr∆∞·ªüng', 'ph√≥', 'tr∆∞·ªùng', 'Tr∆∞·ªüng', 'Ph√≥', 'Tr∆∞·ªùng', 'nh√†', 'b·ªô', 'Hi·ªáu', 'tru', 'tr', 'tr∆∞∆°ng', 'ph·ª•', 'gi√°o', 'ƒë·ªôi', 'tr∆∞∆°', 'h·ªçc', 'ch√≠nh', 'ƒê·∫°i', 'qu·∫£n', 'ban', 'ch·ªß', 'hi·ªáu', 'H·ªçc', 'gi·∫£ngTr', 'c√°c', 'M', 'sinh', '-', 'VƒÉn', 'Tr∆∞∆°ng', 'ch√°n', 'ƒë·∫°i', 'ng∆∞·ªùi', 's√°t', 'Qu·∫£n', 'Ban', 'gi√°m', 'Ch·ªß', 'u', 'S', '\"', 'ƒë·ªÅ', 'th·∫ßy', 'm', 't·ª´', 'ng√†nh,', 's√°ch', 'Thu', 'qu·∫£', 'Hoa', 'thu·ªëc', 'Ph√≤ng', 'ƒë√≠nh', 'n·ªôi', 'cha', 'th∆∞', 'G', 'Gi√°o', 'n√≥i', 'thi·∫øt', 'chu·∫©n', 'Gi√°m', 'vi√™n', 'c√°n', 'To', 'Qu', 'Th', 'Ch√°nh', 'huynh', 'h∆∞·ªõng', 't∆∞·ªùng', 'B·ªô', 'Cao', 'Nh√†', 'Y', 'x', 'vi·ªán', 'nhi·ªám', 't√†i', 'Minh', 'L', 'to√°n', 'L√Ω', 'vƒÉn', 'm·∫´u', 'Ch√≠nh', 'ƒê·ªô', 'thu', 't∆∞·ªüng', 'Ho', '/', 'Huy', 's', 'P', 'cao', '(']\n",
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh <mask> ng√†y t·ª±u tr∆∞·ªùng 20/8 t·ªõi ƒë√¢y.\n",
      "->Top 100 Predictions: ['v√†', 'v√†o,', 'ngay', 'trong', 'to√†n', 'tr∆∞·ªõc', 't·∫°i', 'bi·∫øt', 'l·ªõp', 'tr√™n', 'c·ªßa', 't·ª´', 'c·∫£', 'th√¥ng', 'h·ªçc', 'theo', 'v·ªÅ', 'qua', 'b·∫±ng', 'k·ªãp', 'c≈©ng', '·ªü', 'sau', '(', 'tr∆∞·ªùng', 'ng√†y', 'khi', 'ƒë·∫ßu', 'm·ªôt', 'c·ª•', 'nh√†', 'c√πng', 'c√°c', 'ƒë∆∞·ª£c', 'ch√≠nh', 'ƒë·∫øn', 'h√¥m', 'ƒë√∫ng', 'ch·∫≠m', 'd·ª±', 'tham', 'ƒë·∫ßy', 't·ª±', 'ƒë·ªÉ', 'n·ªôi', 's·ªõm', 'r√µ', 'cu·ªëi', 'ƒëƒÉng', 'Tr∆∞·ªùng', 'c√≥', '-', 'ch·ªß', 'nh√¢n', 'xem', 'n·∫Øm', 'nghe', 'k·ªÉ', 'kh·ªëi', 'nh·∫≠n', 'ghi', 'THPT', 'thu·ªôc', 'ho·∫∑c', '/', 'b·∫Øt', 'l√†m', 'tr·ª±c', 'kh√¥ng', 'HS', 'ph·ªï', 'ƒë·ªìng', 'l√†', 'ch·ªâ', 'sinh', 'mu·ªôn', 't·∫≠p', 'thay', 'c√¥ng', '√≠t', 'm·ªõi', 'r·∫•t', 'trung', 'k√Ω', 'ph·ª•', 'nhanh', 't·ªï', 't·ªõi', 's·∫Ω', 'ƒë·ªãa.', 'nh·ªØng', 'n∆°i', 'hai', 'chi', 't·∫•t', 'ho√†n', 't·ª´ng', 'h√†ng']\n",
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y <mask> tr∆∞·ªùng 20/8 t·ªõi ƒë√¢y.\n",
      "->Top 100 Predictions: ['khai', 't·ª±', 'l·ªÖ', 'k·ª∑', 'Nh√†', 'b·∫ø', 'h·ªôi', 't·ªïng', 'h·ªçp', 'L·ªÖ', 'ngh·ªâ', 'ƒë·∫ßu', 'nh√†', 'ra', 'Ph·ª•', 'ph·ª•', 'sinh', '20', 'Kha', 't·∫°m', 'ph√°t', 'H·ªôi', 'ch√≠nh', 'h·ªçc', 'T·ª±', 'ch', 'to√†n', 'gi', 'truy·ªÅn', 'l√†m', 'th·ª©', 'm·ªü', 'cu·ªëi', 'v·ªÅ', 'vui', 'k', 'Qu·ªëc', 'th√†nh', 's∆°', 'nh·∫≠p', 'Ng√†y', 'chia', 'tuy√™n', 'kh√°', 'Hi', 'T·ªïng', 'ƒë∆∞a', 't·ªï', 'tr·ªü', '\"', 'b·∫Øt', 'tan', 'r', 'Ch·ªß', 'ƒë·∫∑c', 'bi·ªÉu', 'tr·ªçng', 'ch√†o', 'g·∫∑p', '30', 'ch·ªß', '19', 'nh·∫≠n', 'di·ªÖn', 'd·ª±', 'hi·ªáu', 't·ªët', 'K', 'T·∫øt', 'chuy·ªÉn', 'di', 'h√¥m', 't·∫•t', 'g·ª≠i', '15', 'T', 'B·∫£o', 'G', 'Gi', 'T·∫•t', 'x·∫£', 'Hi·ªáu', '25', 'hi·∫øn', 'quay', 'Trung', 'ƒë√≥n', 'Ph√°t', 'k·∫øt', '18', 'r·ªùi', 'b·∫£o', 'thi', 'ng√†y', 'xu·∫•t', 'ƒëi·ªÅu', 'ƒëi', 'th∆∞·ªùng', 'ki·ªÉm']\n",
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng <mask> t·ªõi ƒë√¢y.\n",
      "->Top 100 Predictions: ['20,', '19', '10', '22', '15', '21', '9', 'ng√†y', '27', '25', 'v√†o', '(', '17', '26', '1', '5', '3', '23', '2', '29', '8', '28', '7', '18', '30', '4', '14', '11', '12', '(20', '6', '13', '20.', '31', 'c·ªßa', '24', 'v√†', '-', 'ch√≠nh', '(1', 's·∫Øp', '16', '(10', 'ƒë·∫ßu', 'th√°ng', '(2', '(19', '10.', 'th·ª©', '(5', 'h√¥m', '(27', '(22', '(25', '(12', '(15', 'h·ªçc', '26.', '22.', '(9', '19.', 'cu·ªëi', '(21', '1/8', '15.', '(23', '(17', '27.', '(26', 'd·ª±', '(11', 'l√†', '(28.', '21.', '29.', 'di·ªÖn', '(6', '(24', '1/3', '17.', '09', 'nƒÉm', '28.', '18.', '23.', '25.', '(30', '11.', '12.', '01', '(18', '1.', '(3', '14.', '(29', '(4', '1.9']\n",
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng 20/8 <mask> ƒë√¢y.\n",
      "->Top 100 Predictions: ['t·ªõi./2010,', 'v√†', 's·∫Øp/2011/2009', 'n√†y', 'v·ª´a', 'nƒÉm/2012', 't·∫°i/2017', '-', 'nh∆∞/2018', 'ho·∫∑c', 'c·ªßa/2016/2013', 'nh∆∞ng', 'tr√™n', '(', 'theo/2015', 'm·ªõi/2014', 'c≈©ng', 'thay', 'tr∆∞·ªõc', 'sau', 'ch·ª©', 'c√πng', 'ƒë·ªÉ', 'trong', 'g·∫ßn', '√¢m', 'v·ªõi', 'ƒë·∫øn/', 'h·ªçc', 't·ª©c/2008;/2007', 'th√¥ng', '·ªü', 'hay', 'ƒë√∫ng', '/-2010', 'd∆∞·ªõi', 'nh·∫±m', 'n√™n/2006', 'v√¨', 'khi', 'h√†ng', 'so', 'v√†o', 's·∫Ω', 'ch√≠nh', 'kh√¥ng', 'm√†', 'cho', 'ƒë·ªìng', 'm·ªôt', 'ƒë∆∞·ª£c', 'r·ªìi', 'd·ª±', 'sang', 't·ª´:', 'di·ªÖn...', 't·ªï', 'gi·ªëng', 'ngay', 'd∆∞∆°ng', 'song', 't∆∞∆°ng', 'do', 'd√†nh/2005-2011', 'b·∫±ng', 'n√≥i', 'n·∫øu', 'qua', '\"', 'ti·∫øp', 's·ªõm-2009', 'c√≥', 'gi·ªù', 'c·ª•', 'tr·ªü', '2011', 'ƒë√¢y']\n",
      "Masked Text: Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng 20/8 t·ªõi <mask>\n",
      "->Top 100 Predictions: ['.', 'ƒë√¢y,', 'v√†', '(', 't·∫°i...', 'ƒë·ªÉ', 'tr√™n', 'c·ªßa', 'nh∆∞ng', 'theo', 'ho·∫∑c', '-!', 'c√πng', 'trong', '·ªü', 'n√†y', 'c≈©ng;', 'ch·ª©', 'khi', 'sau', 'nay', 'v·ªõi:', 'h·ªçc', '(16', 'g·∫ßn', 'do', 'nh·∫±m', 'm√†', 'tr∆∞·ªõc', 'nh∆∞', '(3', 'n√™n', '(11', 'l√†', '(5', '(15', '(4', '(28', 'ƒê√¢y', 'ƒë√∫ng', '(10', 'thay', 'v√†o', '(17', 'ƒë√≥', 'd∆∞·ªõi', 'v√¨', 'n·∫øu', '(12', 'ng√†y', '(1', '(24', 'th√¥ng', '(9', '(14', '(6', '(13', '(30', 'ngay', 'cho', 'r·ªìi', 's·∫Ω', '(7', 't·ªõi', '(8', 's·∫Øp', '(22', 'hay', 'th·ªùi', 'v·ª´a', 'c√°c', 'ƒë∆∞·ª£c', 'th√¥i', '(29', '(31', '\"', 'qua', 'd·ª±', '(25', '(18', 'l√†m', '(27', '(21', '(26', '(2', 'c√≥', '(20', '(19', 'h·∫øt', '(23', 'tr∆∞·ªùng', 'nh√¢n', 't·∫≠n', 'ƒë·∫ßy', 'ƒë·ªìng']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng 20/8 t·ªõi ƒë√¢y.',\n",
       "  [(4, 'ƒê·∫£o,', 'Th'),\n",
       "   (6, 'tr∆∞·ªüng,', 'tr∆∞·ªüng'),\n",
       "   (18, 'v√†o', 'v√†'),\n",
       "   (20, 't·ª±u', 'khai'),\n",
       "   (22, '20/8', '20,'),\n",
       "   (23, 't·ªõi', 't·ªõi./2010,'),\n",
       "   (24, 'ƒë√¢y.', '.')])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TXT = \"Trung t√¢m D·ª± b√°o kh√≠ t∆∞·ª£ng th·ªßy vƒÉn qu·ªëc gia cho bi·∫øt l√∫c 7h h√¥m nay, √°p th·∫•p nhi·ªát ƒë·ªõi m·∫°nh 61 km/h, c·∫•p 6-7, \"\n",
    "TXT = \"Theo th·∫ßy ƒê·ªó ƒê√¨nh ƒê·∫£o, Hi·ªáu tr∆∞·ªüng, nh·ªØng ƒëi·ªÅu ch·ªânh n√†y s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o cho h·ªçc sinh v√†o ng√†y t·ª±u tr∆∞·ªùng 20/8 t·ªõi ƒë√¢y.\"\n",
    "paragraph_prediction(TXT, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "887b7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BARTpho-word\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n",
    "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2dfc5eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqModelOutput' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m TXT = \u001b[33m'\u001b[39m\u001b[33mTrung t√¢m D·ª± b√°o kh√≠ t∆∞·ª£ng th·ªßy vƒÉn qu·ªëc gia cho bi·∫øt l√∫c 7h h√¥m nay, √°p th·∫•p nhi·ªát ƒë·ªõi m·∫°nh 61 km/h, c·∫•p 6-7,\u001b[39m\u001b[33m'\u001b[39m  \n\u001b[32m      2\u001b[39m input_ids = word_tokenizer(TXT, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m features = bartpho_word(input_ids).logits\n",
      "\u001b[31mAttributeError\u001b[39m: 'Seq2SeqModelOutput' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "TXT = 'Trung t√¢m D·ª± b√°o kh√≠ t∆∞·ª£ng th·ªßy vƒÉn qu·ªëc gia cho bi·∫øt l√∫c 7h h√¥m nay, √°p th·∫•p nhi·ªát ƒë·ªõi m·∫°nh 61 km/h, c·∫•p 6-7,'  \n",
    "input_ids = word_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_word(input_ids).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e1dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     features = phobert(input_ids)  \u001b[38;5;66;03m# Models outputs are now tuples\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Extract the logits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m logits = features.logits\n",
      "\u001b[31mAttributeError\u001b[39m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"T√¥i l√† <mask> tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed1b381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples\n",
    "\n",
    "# Extract the logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.51025390625,\n",
       "  'token': 3716,\n",
       "  'token_str': 'Hi·ªáu_tr∆∞·ªüng',\n",
       "  'sequence': 'T√¥i l√† Hi·ªáu_tr∆∞·ªüng tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.2998046875,\n",
       "  'token': 4856,\n",
       "  'token_str': 'hi·ªáu_tr∆∞·ªüng',\n",
       "  'sequence': 'T√¥i l√† hi·ªáu_tr∆∞·ªüng tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.039947509765625,\n",
       "  'token': 3466,\n",
       "  'token_str': 'gi·∫£ng_vi√™n',\n",
       "  'sequence': 'T√¥i l√† gi·∫£ng_vi√™n tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.03753662109375,\n",
       "  'token': 250,\n",
       "  'token_str': 'Ch·ªß_t·ªãch',\n",
       "  'sequence': 'T√¥i l√† Ch·ªß_t·ªãch tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.0199432373046875,\n",
       "  'token': 649,\n",
       "  'token_str': 'sinh_vi√™n',\n",
       "  'sequence': 'T√¥i l√† sinh_vi√™n tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.00891876220703125,\n",
       "  'token': 1623,\n",
       "  'token_str': 'ch·ªß_t·ªãch',\n",
       "  'sequence': 'T√¥i l√† ch·ªß_t·ªãch tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.0084381103515625,\n",
       "  'token': 693,\n",
       "  'token_str': 'Gi√°m_ƒë·ªëc',\n",
       "  'sequence': 'T√¥i l√† Gi√°m_ƒë·ªëc tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.007450103759765625,\n",
       "  'token': 3332,\n",
       "  'token_str': 'gi√°o_s∆∞',\n",
       "  'sequence': 'T√¥i l√† gi√°o_s∆∞ tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.004245758056640625,\n",
       "  'token': 30929,\n",
       "  'token_str': 'c·ª±u_h·ªçc_sinh',\n",
       "  'sequence': 'T√¥i l√† c·ª±u_h·ªçc_sinh tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'},\n",
       " {'score': 0.003688812255859375,\n",
       "  'token': 33952,\n",
       "  'token_str': 'Hi·ªáu_ph√≥',\n",
       "  'sequence': 'T√¥i l√† Hi·ªáu_ph√≥ tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá .'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12b55796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tr∆∞·ªùng ƒë·∫°i_h·ªçc b√°ch_khoa h√†_n·ªôi'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "ViTokenizer.tokenize(u\"Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ca4ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace u·ª∑ with ·ªßy\n",
    "dict_map = {\n",
    "    \"√≤a\": \"o√†\",\n",
    "    \"√ía\": \"O√†\",\n",
    "    \"√íA\": \"O√Ä\",\n",
    "    \"√≥a\": \"o√°\",\n",
    "    \"√ìa\": \"O√°\",\n",
    "    \"√ìA\": \"O√Å\",\n",
    "    \"·ªèa\": \"o·∫£\",\n",
    "    \"·ªéa\": \"O·∫£\",\n",
    "    \"·ªéA\": \"O·∫¢\",\n",
    "    \"√µa\": \"o√£\",\n",
    "    \"√ïa\": \"O√£\",\n",
    "    \"√ïA\": \"O√É\",\n",
    "    \"·ªça\": \"o·∫°\",\n",
    "    \"·ªåa\": \"O·∫°\",\n",
    "    \"·ªåA\": \"O·∫†\",\n",
    "    \"√≤e\": \"o√®\",\n",
    "    \"√íe\": \"O√®\",\n",
    "    \"√íE\": \"O√à\",\n",
    "    \"√≥e\": \"o√©\",\n",
    "    \"√ìe\": \"O√©\",\n",
    "    \"√ìE\": \"O√â\",\n",
    "    \"·ªèe\": \"o·∫ª\",\n",
    "    \"·ªée\": \"O·∫ª\",\n",
    "    \"·ªéE\": \"O·∫∫\",\n",
    "    \"√µe\": \"o·∫Ω\",\n",
    "    \"√ïe\": \"O·∫Ω\",\n",
    "    \"√ïE\": \"O·∫º\",\n",
    "    \"·ªçe\": \"o·∫π\",\n",
    "    \"·ªåe\": \"O·∫π\",\n",
    "    \"·ªåE\": \"O·∫∏\",\n",
    "    \"√πy\": \"u·ª≥\",\n",
    "    \"√ôy\": \"U·ª≥\",\n",
    "    \"√ôY\": \"U·ª≤\",\n",
    "    \"√∫y\": \"u√Ω\",\n",
    "    \"√öy\": \"U√Ω\",\n",
    "    \"√öY\": \"U√ù\",\n",
    "    \"·ªßy\": \"u·ª∑\",\n",
    "    \"·ª¶y\": \"U·ª∑\",\n",
    "    \"·ª¶Y\": \"U·ª∂\",\n",
    "    \"≈©y\": \"u·ªπ\",\n",
    "    \"≈®y\": \"U·ªπ\",\n",
    "    \"≈®Y\": \"U·ª∏\",\n",
    "    \"·ª•y\": \"u·ªµ\",\n",
    "    \"·ª§y\": \"U·ªµ\",\n",
    "    \"·ª§Y\": \"U·ª¥\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT = \"Trung t√¢m D·ª± b√°o Kh√≠ t∆∞·ª£ng Th·ªßy vƒÉn qu·ªëc gia cho bi·∫øt l√∫c 7h h√¥m na, √°p th·∫•p nhi·ªát ƒë·ªõi m·∫°nh 61 km/h, c·∫•p 6-7,\"\n",
    "# replace u·ª∑ with ·ªßy\n",
    "dict_map = {\n",
    "    \"√≤a\": \"o√†\",\n",
    "    \"√ía\": \"O√†\",\n",
    "    \"√íA\": \"O√Ä\",\n",
    "    \"√≥a\": \"o√°\",\n",
    "    \"√ìa\": \"O√°\",\n",
    "    \"√ìA\": \"O√Å\",\n",
    "    \"·ªèa\": \"o·∫£\",\n",
    "    \"·ªéa\": \"O·∫£\",\n",
    "    \"·ªéA\": \"O·∫¢\",\n",
    "    \"√µa\": \"o√£\",\n",
    "    \"√ïa\": \"O√£\",\n",
    "    \"√ïA\": \"O√É\",\n",
    "    \"·ªça\": \"o·∫°\",\n",
    "    \"·ªåa\": \"O·∫°\",\n",
    "    \"·ªåA\": \"O·∫†\",\n",
    "    \"√≤e\": \"o√®\",\n",
    "    \"√íe\": \"O√®\",\n",
    "    \"√íE\": \"O√à\",\n",
    "    \"√≥e\": \"o√©\",\n",
    "    \"√ìe\": \"O√©\",\n",
    "    \"√ìE\": \"O√â\",\n",
    "    \"·ªèe\": \"o·∫ª\",\n",
    "    \"·ªée\": \"O·∫ª\",\n",
    "    \"·ªéE\": \"O·∫∫\",\n",
    "    \"√µe\": \"o·∫Ω\",\n",
    "    \"√ïe\": \"O·∫Ω\",\n",
    "    \"√ïE\": \"O·∫º\",\n",
    "    \"·ªçe\": \"o·∫π\",\n",
    "    \"·ªåe\": \"O·∫π\",\n",
    "    \"·ªåE\": \"O·∫∏\",\n",
    "    \"√πy\": \"u·ª≥\",\n",
    "    \"√ôy\": \"U·ª≥\",\n",
    "    \"√ôY\": \"U·ª≤\",\n",
    "    \"√∫y\": \"u√Ω\",\n",
    "    \"√öy\": \"U√Ω\",\n",
    "    \"√öY\": \"U√ù\",\n",
    "    \"·ªßy\": \"u·ª∑\",\n",
    "    \"·ª¶y\": \"U·ª∑\",\n",
    "    \"·ª¶Y\": \"U·ª∂\",\n",
    "    \"≈©y\": \"u·ªπ\",\n",
    "    \"≈®y\": \"U·ªπ\",\n",
    "    \"≈®Y\": \"U·ª∏\",\n",
    "    \"·ª•y\": \"u·ªµ\",\n",
    "    \"·ª§y\": \"U·ªµ\",\n",
    "    \"·ª§Y\": \"U·ª¥\",\n",
    "    }\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "ViTokenizer.tokenize(u\"Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi\")\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "def process_text(text):\n",
    "    for key, value in dict_map.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def sentence_prediction(text, top_k=30):\n",
    "    text = ViTokenizer.tokenize(TXT)    \n",
    "    \n",
    "    error_indices = []\n",
    "    for i in range(len(text.split())):\n",
    "        text = process_text(text)\n",
    "        new_list = text.split()\n",
    "        original_word = new_list[i]\n",
    "        if is_number(original_word):\n",
    "            continue\n",
    "        \n",
    "        new_list[i] = tokenizer.mask_token\n",
    "        masked_text = \" \".join(new_list)\n",
    "        start_time = time.time()\n",
    "        predictions = pipeline(masked_text, top_k=100)\n",
    "        topk = []\n",
    "        for item in predictions:\n",
    "            if 'token_str' in item:\n",
    "                topk.append(item['token_str'])\n",
    "        end_time = time.time()\n",
    "        if original_word not in topk:\n",
    "            print(f\"Original Word: {original_word}\")\n",
    "            print(f\"Masked Text: {masked_text}\")\n",
    "            print(f\"->Top 10 Predictions: {topk}\")\n",
    "            error_indices.append((i, original_word, topk[0]))\n",
    "        # print(f\"Masked Text: {masked_text}\")\n",
    "    return error_indices\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fadc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tr∆∞·ªõc ƒëy , gi√° ƒë·∫•t ·ªü khu_v·ª±c n√†y ch·ªâ 2 , 7 tri·ªáu ƒë·ªìng m·ªói m2 .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "example = \"\"\"\n",
    "Tr∆∞·ªõc ƒëy, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.\n",
    "\"\"\"\n",
    "ner_results = nlp(example)\n",
    "example_tok = \"\"\n",
    "for e in ner_results:\n",
    "    if \"##\" in e[\"word\"]:\n",
    "        example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "    elif e[\"entity\"] ==\"I\":\n",
    "        example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "    else:\n",
    "        example_tok = example_tok + \" \" + e[\"word\"]\n",
    "print(example_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe6685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Tr∆∞·ªõc ƒëy, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.\n",
      "Segmented: Tr∆∞·ªõc ƒëy , gi√° ƒë·∫•t ·ªü khu_v·ª±c n√†y ch·ªâ 2 , 7 tri·ªáu ƒë·ªìng m·ªói m2 .\n",
      "\n",
      "Sentence 1:\n",
      "Original: Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi\n",
      "Segmented: Tr∆∞·ªùng ƒë·∫°i_h·ªçc b√°ch_khoa h√†_n·ªôi\n",
      "\n",
      "Sentence 2:\n",
      "Original: Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß Ph·∫°m Minh Ch√≠nh\n",
      "Segmented: Th·ªß_t∆∞·ªõng Ch√≠nh_ph·ªß Ph·∫°m_Minh_Ch√≠nh\n",
      "\n",
      "Sentence 3:\n",
      "Original: Ng√¢n h√†ng nh√† n∆∞·ªõc Vi·ªát Nam\n",
      "Segmented: Ng√¢n_h√†ng nh√†_n∆∞·ªõc Vi·ªát_Nam\n"
     ]
    }
   ],
   "source": [
    "class VietnameseWordSegmenter:\n",
    "    \"\"\"\n",
    "    A Vietnamese word segmentation class using NlpHUST/vi-word-segmentation model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"NlpHUST/vi-word-segmentation\"):\n",
    "        \"\"\"\n",
    "        Initialize the Vietnamese word segmenter\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The pre-trained model name from HuggingFace\n",
    "        \"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.nlp = pipeline(\"token-classification\", model=self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "    def segment_text(self, text):\n",
    "        \"\"\"\n",
    "        Segment Vietnamese text into words\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input Vietnamese text to be segmented\n",
    "            \n",
    "        Returns:\n",
    "            str: Segmented text with underscores connecting compound words\n",
    "        \"\"\"\n",
    "        # Clean the input text\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Get NER results\n",
    "        ner_results = self.nlp(text)\n",
    "        \n",
    "        # Process the results to create segmented text\n",
    "        segmented_text = \"\"\n",
    "        for entity in ner_results:\n",
    "            word = entity[\"word\"]\n",
    "            entity_label = entity[\"entity\"]\n",
    "            \n",
    "            if \"##\" in word:\n",
    "                # Handle subword tokens (BERT tokenization artifacts)\n",
    "                segmented_text += word.replace(\"##\", \"\")\n",
    "            elif entity_label == \"I\":\n",
    "                # Inside entity - connect with underscore\n",
    "                segmented_text += \"_\" + word\n",
    "            else:\n",
    "                # Beginning of entity or outside entity - add space\n",
    "                segmented_text += \" \" + word\n",
    "                \n",
    "        return segmented_text.strip()\n",
    "    \n",
    "    def segment_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Segment multiple sentences\n",
    "        \n",
    "        Args:\n",
    "            sentences (list): List of Vietnamese sentences\n",
    "            \n",
    "        Returns:\n",
    "            list: List of segmented sentences\n",
    "        \"\"\"\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "            \n",
    "        segmented_sentences = []\n",
    "        for sentence in sentences:\n",
    "            segmented = self.segment_text(sentence)\n",
    "            segmented_sentences.append(segmented)\n",
    "            \n",
    "        return segmented_sentences\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Allow the class to be called directly\n",
    "        \n",
    "        Args:\n",
    "            text (str or list): Input text(s) to segment\n",
    "            \n",
    "        Returns:\n",
    "            str or list: Segmented text(s)\n",
    "        \"\"\"\n",
    "        if isinstance(text, list):\n",
    "            return self.segment_sentences(text)\n",
    "        else:\n",
    "            return self.segment_text(text)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the segmenter\n",
    "    segmenter = VietnameseWordSegmenter()\n",
    "    \n",
    "    # Test with example text\n",
    "    example_text = \"Tr∆∞·ªõc ƒëy, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.\"\n",
    "    segmented_result = segmenter.segment_text(example_text)\n",
    "    print(f\"Original: {example_text}\")\n",
    "    print(f\"Segmented: {segmented_result}\")\n",
    "    \n",
    "    # Test with multiple sentences\n",
    "    sentences = [\n",
    "        \"Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi\",\n",
    "        \"Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß Ph·∫°m Minh Ch√≠nh\",\n",
    "        \"Ng√¢n h√†ng nh√† n∆∞·ªõc Vi·ªát Nam\"\n",
    "    ]\n",
    "    \n",
    "    segmented_sentences = segmenter.segment_sentences(sentences)\n",
    "    for i, (original, segmented) in enumerate(zip(sentences, segmented_sentences)):\n",
    "        print(f\"\\nSentence {i+1}:\")\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"Segmented: {segmented}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58666c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese Word Segmenter API is ready!\n",
      "To start the server, run: run_server()\n",
      "Or run: app.run(host='0.0.0.0', port=5000, debug=True)\n"
     ]
    }
   ],
   "source": [
    "# Flask API Interface\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize the segmenter (global instance)\n",
    "segmenter = None\n",
    "\n",
    "def initialize_segmenter():\n",
    "    \"\"\"Initialize the segmenter if not already initialized\"\"\"\n",
    "    global segmenter\n",
    "    if segmenter is None:\n",
    "        print(\"Initializing Vietnamese Word Segmenter...\")\n",
    "        segmenter = VietnameseWordSegmenter()\n",
    "        print(\"Segmenter initialized successfully!\")\n",
    "\n",
    "@app.route('/api/segment', methods=['POST'])\n",
    "def segment_text_api():\n",
    "    \"\"\"\n",
    "    API endpoint to segment Vietnamese text\n",
    "    \n",
    "    Expected JSON payload:\n",
    "    {\n",
    "        \"text\": \"Your Vietnamese text here\"\n",
    "    }\n",
    "    \n",
    "    Returns:\n",
    "    {\n",
    "        \"original\": \"original text\",\n",
    "        \"segmented\": \"segmented text\",\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize segmenter if needed\n",
    "        initialize_segmenter()\n",
    "        \n",
    "        # Get JSON data from request\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'text' not in data:\n",
    "            return jsonify({\n",
    "                'error': 'No text provided. Please send JSON with \"text\" field.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        input_text = data['text'].strip()\n",
    "        \n",
    "        if not input_text:\n",
    "            return jsonify({\n",
    "                'error': 'Empty text provided.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        # Perform segmentation\n",
    "        segmented_text = segmenter.segment_text(input_text)\n",
    "        \n",
    "        # Return results\n",
    "        return jsonify({\n",
    "            'original': input_text,\n",
    "            'segmented': segmented_text,\n",
    "            'status': 'success'\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'An error occurred: {str(e)}',\n",
    "            'status': 'error'\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/segment/batch', methods=['POST'])\n",
    "def segment_batch_api():\n",
    "    \"\"\"\n",
    "    API endpoint to segment multiple Vietnamese sentences\n",
    "    \n",
    "    Expected JSON payload:\n",
    "    {\n",
    "        \"texts\": [\"sentence 1\", \"sentence 2\", ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        initialize_segmenter()\n",
    "        \n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'texts' not in data:\n",
    "            return jsonify({\n",
    "                'error': 'No texts provided. Please send JSON with \"texts\" field.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        input_texts = data['texts']\n",
    "        \n",
    "        if not isinstance(input_texts, list):\n",
    "            return jsonify({\n",
    "                'error': 'texts field must be a list of strings.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        # Perform batch segmentation\n",
    "        segmented_texts = segmenter.segment_sentences(input_texts)\n",
    "        \n",
    "        # Return results\n",
    "        return jsonify({\n",
    "            'original': input_texts,\n",
    "            'segmented': segmented_texts,\n",
    "            'status': 'success'\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'An error occurred: {str(e)}',\n",
    "            'status': 'error'\n",
    "        }), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'service': 'Vietnamese Word Segmenter API',\n",
    "        'segmenter_initialized': segmenter is not None\n",
    "    })\n",
    "\n",
    "@app.route('/')\n",
    "def web_interface():\n",
    "    \"\"\"Web interface for the segmenter\"\"\"\n",
    "    return render_template_string(WEB_TEMPLATE)\n",
    "\n",
    "# Web interface HTML template\n",
    "WEB_TEMPLATE = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Vietnamese Word Segmenter</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        .container {\n",
    "            background: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .input-group {\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        label {\n",
    "            display: block;\n",
    "            margin-bottom: 8px;\n",
    "            font-weight: bold;\n",
    "            color: #555;\n",
    "        }\n",
    "        textarea {\n",
    "            width: 100%;\n",
    "            padding: 12px;\n",
    "            border: 2px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "            font-size: 14px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            resize: vertical;\n",
    "            min-height: 150px;\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        textarea:focus {\n",
    "            border-color: #4CAF50;\n",
    "            outline: none;\n",
    "        }\n",
    "        .button-group {\n",
    "            text-align: center;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        button {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "            padding: 12px 30px;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            cursor: pointer;\n",
    "            font-size: 16px;\n",
    "            margin: 0 10px;\n",
    "        }\n",
    "        button:hover {\n",
    "            background-color: #45a049;\n",
    "        }\n",
    "        button:disabled {\n",
    "            background-color: #ccc;\n",
    "            cursor: not-allowed;\n",
    "        }\n",
    "        .clear-btn {\n",
    "            background-color: #f44336;\n",
    "        }\n",
    "        .clear-btn:hover {\n",
    "            background-color: #da190b;\n",
    "        }\n",
    "        .output-area {\n",
    "            background-color: #f8f8f8;\n",
    "            border: 2px solid #eee;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .status {\n",
    "            text-align: center;\n",
    "            padding: 10px;\n",
    "            margin: 10px 0;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .status.success {\n",
    "            background-color: #d4edda;\n",
    "            color: #155724;\n",
    "        }\n",
    "        .status.error {\n",
    "            background-color: #f8d7da;\n",
    "            color: #721c24;\n",
    "        }\n",
    "        .status.loading {\n",
    "            background-color: #cce7ff;\n",
    "            color: #004085;\n",
    "        }\n",
    "        .example-texts {\n",
    "            background-color: #e7f3ff;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .example-text {\n",
    "            cursor: pointer;\n",
    "            color: #0066cc;\n",
    "            text-decoration: underline;\n",
    "            margin: 5px 0;\n",
    "            display: block;\n",
    "        }\n",
    "        .example-text:hover {\n",
    "            color: #004499;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>üáªüá≥ Vietnamese Word Segmenter</h1>\n",
    "        \n",
    "        <div class=\"example-texts\">\n",
    "            <h3>üìù Example texts (click to use):</h3>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi')\">Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi</span>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß Ph·∫°m Minh Ch√≠nh')\">Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß Ph·∫°m Minh Ch√≠nh</span>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Ng√¢n h√†ng nh√† n∆∞·ªõc Vi·ªát Nam')\">Ng√¢n h√†ng nh√† n∆∞·ªõc Vi·ªát Nam</span>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Tr∆∞·ªõc ƒë√¢y, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.')\">Tr∆∞·ªõc ƒë√¢y, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.</span>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-group\">\n",
    "            <label for=\"inputText\">Original Text:</label>\n",
    "            <textarea id=\"inputText\" placeholder=\"Enter Vietnamese text here...\"></textarea>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"button-group\">\n",
    "            <button onclick=\"segmentText()\">üîç Segment Text</button>\n",
    "            <button class=\"clear-btn\" onclick=\"clearText()\">üóëÔ∏è Clear All</button>\n",
    "        </div>\n",
    "        \n",
    "        <div id=\"status\"></div>\n",
    "        \n",
    "        <div class=\"input-group\">\n",
    "            <label for=\"outputText\">Segmented Text:</label>\n",
    "            <textarea id=\"outputText\" class=\"output-area\" readonly placeholder=\"Segmented text will appear here...\"></textarea>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        function setInputText(text) {\n",
    "            document.getElementById('inputText').value = text;\n",
    "        }\n",
    "        \n",
    "        function clearText() {\n",
    "            document.getElementById('inputText').value = '';\n",
    "            document.getElementById('outputText').value = '';\n",
    "            document.getElementById('status').innerHTML = '';\n",
    "        }\n",
    "        \n",
    "        function showStatus(message, type) {\n",
    "            const statusDiv = document.getElementById('status');\n",
    "            statusDiv.innerHTML = message;\n",
    "            statusDiv.className = `status ${type}`;\n",
    "        }\n",
    "        \n",
    "        async function segmentText() {\n",
    "            const inputText = document.getElementById('inputText').value.trim();\n",
    "            const outputText = document.getElementById('outputText');\n",
    "            \n",
    "            if (!inputText) {\n",
    "                showStatus('Please enter some text to segment.', 'error');\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Show loading status\n",
    "            showStatus('Processing text...', 'loading');\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch('/api/segment', {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify({\n",
    "                        text: inputText\n",
    "                    })\n",
    "                });\n",
    "                \n",
    "                const result = await response.json();\n",
    "                \n",
    "                if (result.status === 'success') {\n",
    "                    outputText.value = result.segmented;\n",
    "                    showStatus('‚úÖ Text segmented successfully!', 'success');\n",
    "                } else {\n",
    "                    showStatus(`‚ùå Error: ${result.error}`, 'error');\n",
    "                    outputText.value = '';\n",
    "                }\n",
    "                \n",
    "            } catch (error) {\n",
    "                showStatus(`‚ùå Error: ${error.message}`, 'error');\n",
    "                outputText.value = '';\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Allow Enter key to trigger segmentation (with Shift+Enter for new line)\n",
    "        document.getElementById('inputText').addEventListener('keydown', function(event) {\n",
    "            if (event.key === 'Enter' && !event.shiftKey) {\n",
    "                event.preventDefault();\n",
    "                segmentText();\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Function to run the server\n",
    "def run_server(host='0.0.0.0', port=5000, debug=True):\n",
    "    \"\"\"\n",
    "    Run the Flask server\n",
    "    \n",
    "    Args:\n",
    "        host (str): Host address\n",
    "        port (int): Port number\n",
    "        debug (bool): Enable debug mode\n",
    "    \"\"\"\n",
    "    print(f\"Starting Vietnamese Word Segmenter API server...\")\n",
    "    print(f\"Web interface will be available at: http://localhost:{port}\")\n",
    "    print(f\"API endpoint: http://localhost:{port}/api/segment\")\n",
    "    \n",
    "    app.run(host=host, port=port, debug=debug)\n",
    "\n",
    "# Example of how to run the server\n",
    "print(\"Vietnamese Word Segmenter API is ready!\")\n",
    "print(\"To start the server, run: run_server()\")\n",
    "print(\"Or run: app.run(host='0.0.0.0', port=5000, debug=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server\n",
    "# Note: This will run the server and block the cell execution\n",
    "# The web interface will be available at http://localhost:5000\n",
    "# API endpoint will be available at http://localhost:5000/api/segment\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the server\n",
    "    run_server(host='0.0.0.0', port=5000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433b67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseWordSegmenter:\n",
    "    \"\"\"\n",
    "    A class for Vietnamese word segmentation using transformers.\n",
    "    \n",
    "    This class provides methods to segment Vietnamese text into words,\n",
    "    handling subword tokens and word boundaries properly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"NlpHUST/vi-word-segmentation\"):\n",
    "        \"\"\"\n",
    "        Initialize the Vietnamese Word Segmenter.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained model to use.\n",
    "                            Default is \"NlpHUST/vi-word-segmentation\"\n",
    "        \"\"\"\n",
    "        print(f\"Loading Vietnamese Word Segmentation model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.pipeline = pipeline(\"token-classification\", \n",
    "                                model=self.model, \n",
    "                                tokenizer=self.tokenizer)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def segment(self, text):\n",
    "        \"\"\"\n",
    "        Segment Vietnamese text into words.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The Vietnamese text to segment\n",
    "            \n",
    "        Returns:\n",
    "            str: The segmented text with words separated by spaces and \n",
    "                 compound words connected by underscores\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Get token classification results\n",
    "        ner_results = self.pipeline(text.strip())\n",
    "        \n",
    "        # Process the results to create segmented text\n",
    "        segmented_text = self._process_tokens(ner_results)\n",
    "        \n",
    "        return segmented_text.strip()\n",
    "    \n",
    "    def _process_tokens(self, ner_results):\n",
    "        \"\"\"\n",
    "        Process the token classification results to create segmented text.\n",
    "        \n",
    "        Args:\n",
    "            ner_results (list): List of token classification results\n",
    "            \n",
    "        Returns:\n",
    "            str: Processed segmented text\n",
    "        \"\"\"\n",
    "        segmented_text = \"\"\n",
    "        \n",
    "        for token_info in ner_results:\n",
    "            word = token_info[\"word\"]\n",
    "            entity = token_info[\"entity\"]\n",
    "            \n",
    "            if \"##\" in word:\n",
    "                # Handle subword tokens (remove ## prefix)\n",
    "                segmented_text += word.replace(\"##\", \"\")\n",
    "            elif entity == \"I\":\n",
    "                # Inside entity - connect with underscore\n",
    "                segmented_text += \"_\" + word\n",
    "            else:\n",
    "                # Beginning of entity or outside entity - add space\n",
    "                segmented_text += \" \" + word\n",
    "        \n",
    "        return segmented_text\n",
    "    \n",
    "    def segment_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Segment multiple texts at once.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of Vietnamese texts to segment\n",
    "            \n",
    "        Returns:\n",
    "            list: List of segmented texts\n",
    "        \"\"\"\n",
    "        return [self.segment(text) for text in texts]\n",
    "    \n",
    "    def get_word_boundaries(self, text):\n",
    "        \"\"\"\n",
    "        Get detailed word boundary information.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The Vietnamese text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            list: List of dictionaries containing word information\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        \n",
    "        ner_results = self.pipeline(text.strip())\n",
    "        word_info = []\n",
    "        current_word = \"\"\n",
    "        current_start = 0\n",
    "        \n",
    "        for i, token_info in enumerate(ner_results):\n",
    "            word = token_info[\"word\"]\n",
    "            entity = token_info[\"entity\"]\n",
    "            start = token_info.get(\"start\", 0)\n",
    "            end = token_info.get(\"end\", 0)\n",
    "            \n",
    "            if \"##\" in word:\n",
    "                current_word += word.replace(\"##\", \"\")\n",
    "            elif entity == \"I\":\n",
    "                current_word += \"_\" + word\n",
    "            else:\n",
    "                # Save previous word if exists\n",
    "                if current_word:\n",
    "                    word_info.append({\n",
    "                        \"word\": current_word.strip(),\n",
    "                        \"start\": current_start,\n",
    "                        \"end\": end\n",
    "                    })\n",
    "                \n",
    "                # Start new word\n",
    "                current_word = word\n",
    "                current_start = start\n",
    "        \n",
    "        # Add the last word\n",
    "        if current_word:\n",
    "            word_info.append({\n",
    "                \"word\": current_word.strip(),\n",
    "                \"start\": current_start,\n",
    "                \"end\": ner_results[-1].get(\"end\", len(text)) if ner_results else len(text)\n",
    "            })\n",
    "        \n",
    "        return word_info\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Make the class callable - same as segment method.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The Vietnamese text to segment\n",
    "            \n",
    "        Returns:\n",
    "            str: The segmented text\n",
    "        \"\"\"\n",
    "        return self.segment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13219db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vietnamese Word Segmentation model: NlpHUST/vi-word-segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "=== Vietnamese Word Segmentation Demo ===\n",
      "\n",
      "Example 1:\n",
      "Original: Theo b·∫£ng gi√° ƒë·∫•t hi·ªán h√†nh, ƒë·∫•t ·ªü h·∫ªm s√¢u ·ªü ƒë∆∞·ªùng Nguy·ªÖn Xi·ªÉn c√≥ gi√° 28 tri·ªáu ƒë·ªìng m·ªói m2, trong khi ƒë·∫•t n√¥ng nghi·ªáp ch·ªâ kho·∫£ng 600.000 ƒë·ªìng, ch√™nh h∆°n 27 tri·ªáu ƒë·ªìng m·ªói m2, khi·∫øn chi ph√≠ chuy·ªÉn ƒë·ªïi tƒÉng tr√™n 10 l·∫ßn. Tr∆∞·ªõc ƒë√¢y, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.\n",
      "Segmented: Theo b·∫£ng gi√° ƒë·∫•t hi·ªán_h√†nh , ƒë·∫•t ·ªü h·∫ªm s√¢u ·ªü ƒë∆∞·ªùng Nguy·ªÖn_Xi·ªÉn c√≥_gi√° 28 tri·ªáu ƒë·ªìng m·ªói m2 , trong khi ƒë·∫•t n√¥ng_nghi·ªáp ch·ªâ kho·∫£ng 600 . 000 ƒë·ªìng , ch√™nh h∆°n 27 tri·ªáu ƒë·ªìng m·ªói m2 , khi·∫øn chi_ph√≠ chuy·ªÉn_ƒë·ªïi tƒÉng tr√™n 10 l·∫ßn . Tr∆∞·ªõc_ƒë√¢y , gi√° ƒë·∫•t ·ªü khu_v·ª±c n√†y ch·ªâ 2 , 7 tri·ªáu ƒë·ªìng m·ªói m2 .\n",
      "Word boundaries: 63 words found\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Original: Kh√¥ng ri√™ng ch·ªã H·∫°nh, nhi·ªÅu ng∆∞·ªùi d√¢n ph·∫£i rao b√°n nh√† ƒë·∫•t gi√° r·∫ª do kh√¥ng kham n·ªïi nghƒ©a v·ª• t√†i ch√≠nh khi chuy·ªÉn m·ª•c ƒë√≠ch. √îng L√™ VƒÉn Quy·∫øt (ph∆∞·ªùng Long B√¨nh) c√≥ th·ª≠a ƒë·∫•t 210 m2 ƒë·ªß ƒëi·ªÅu ki·ªán chuy·ªÉn th·ªï c∆∞ t·∫°i ph∆∞·ªùng Long B√¨nh, TP HCM nh∆∞ng v∆∞·ªõng quy ho·∫°ch. Khi ƒë∆∞·ª£c g·ª° v∆∞·ªõng, √¥ng Quy·∫øt t√¨m hi·ªÉu th·ªß t·ª•c t√°ch th·ª≠a v√† chuy·ªÉn m·ª•c ƒë√≠ch th√¨ r·∫•t s·ªëc v√¨ ph·∫£i ƒë√≥ng h∆°n 5,7 t·ª∑ ƒë·ªìng ti·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, v∆∞·ª£t kh·∫£ nƒÉng chi tr·∫£.\n",
      "Segmented: Kh√¥ng ri√™ng ch·ªã H·∫°nh , nhi·ªÅu ng∆∞·ªùi_d√¢n ph·∫£i rao b√°n nh√†_ƒë·∫•t gi√° r·∫ª do kh√¥ng kham n·ªïi nghƒ©a_v·ª• t√†i_ch√≠nh khi chuy·ªÉn m·ª•c_ƒë√≠ch . √îng L√™_VƒÉn_Quy·∫øt ( ph∆∞·ªùng Long_B√¨nh ) c√≥ th·ª≠a ƒë·∫•t 210 m2 ƒë·ªß ƒëi·ªÅu_ki·ªán chuy·ªÉn th·ªï_c∆∞ t·∫°i ph∆∞·ªùng Long_B√¨nh , TP HCM nh∆∞ng v∆∞·ªõng quy_ho·∫°ch . Khi ƒë∆∞·ª£c g·ª° v∆∞·ªõng , √¥ng Quy·∫øt t√¨m_hi·ªÉu th·ªß_t·ª•c t√°ch th·ª≠a v√† chuy·ªÉn m·ª•c_ƒë√≠ch th√¨ r·∫•t s·ªëc v√¨ ph·∫£i ƒë√≥ng h∆°n 5 , 7 t·ª∑ ƒë·ªìng ti·ªÅn s·ª≠_d·ª•ng ƒë·∫•t , v∆∞·ª£t kh·∫£_nƒÉng chi_tr·∫£ .\n",
      "Word boundaries: 82 words found\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Original: Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi l√† m·ªôt trong nh·ªØng tr∆∞·ªùng h√†ng ƒë·∫ßu\n",
      "Segmented: Tr∆∞·ªùng ƒë·∫°i_h·ªçc b√°ch_khoa h√†_n·ªôi l√† m·ªôt trong nh·ªØng tr∆∞·ªùng h√†ng_ƒë·∫ßu\n",
      "Word boundaries: 10 words found\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Batch Processing ===\n",
      "Batch 1: Theo b·∫£ng gi√° ƒë·∫•t hi·ªán_h√†nh , ƒë·∫•t ·ªü h·∫ªm s√¢u ·ªü ƒë∆∞·ªùng Nguy·ªÖn_Xi·ªÉn c√≥_gi√° 28 tri·ªáu ƒë·ªìng m·ªói m2 , trong khi ƒë·∫•t n√¥ng_nghi·ªáp ch·ªâ kho·∫£ng 600 . 000 ƒë·ªìng , ch√™nh h∆°n 27 tri·ªáu ƒë·ªìng m·ªói m2 , khi·∫øn chi_ph√≠ chuy·ªÉn_ƒë·ªïi tƒÉng tr√™n 10 l·∫ßn . Tr∆∞·ªõc_ƒë√¢y , gi√° ƒë·∫•t ·ªü khu_v·ª±c n√†y ch·ªâ 2 , 7 tri·ªáu ƒë·ªìng m·ªói m2 .\n",
      "Batch 2: Kh√¥ng ri√™ng ch·ªã H·∫°nh , nhi·ªÅu ng∆∞·ªùi_d√¢n ph·∫£i rao b√°n nh√†_ƒë·∫•t gi√° r·∫ª do kh√¥ng kham n·ªïi nghƒ©a_v·ª• t√†i_ch√≠nh khi chuy·ªÉn m·ª•c_ƒë√≠ch . √îng L√™_VƒÉn_Quy·∫øt ( ph∆∞·ªùng Long_B√¨nh ) c√≥ th·ª≠a ƒë·∫•t 210 m2 ƒë·ªß ƒëi·ªÅu_ki·ªán chuy·ªÉn th·ªï_c∆∞ t·∫°i ph∆∞·ªùng Long_B√¨nh , TP HCM nh∆∞ng v∆∞·ªõng quy_ho·∫°ch . Khi ƒë∆∞·ª£c g·ª° v∆∞·ªõng , √¥ng Quy·∫øt t√¨m_hi·ªÉu th·ªß_t·ª•c t√°ch th·ª≠a v√† chuy·ªÉn m·ª•c_ƒë√≠ch th√¨ r·∫•t s·ªëc v√¨ ph·∫£i ƒë√≥ng h∆°n 5 , 7 t·ª∑ ƒë·ªìng ti·ªÅn s·ª≠_d·ª•ng ƒë·∫•t , v∆∞·ª£t kh·∫£_nƒÉng chi_tr·∫£ .\n",
      "Batch 3: Tr∆∞·ªùng ƒë·∫°i_h·ªçc b√°ch_khoa h√†_n·ªôi l√† m·ªôt trong nh·ªØng tr∆∞·ªùng h√†ng_ƒë·∫ßu\n",
      "\n",
      "=== Using as Callable ===\n",
      "Callable result: T√¥i y√™u Vi·ªát_Nam\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the VietnameseWordSegmenter class\n",
    "\n",
    "# Initialize the segmenter\n",
    "segmenter = VietnameseWordSegmenter()\n",
    "\n",
    "# Example texts to segment\n",
    "examples = [\n",
    "    \"Theo b·∫£ng gi√° ƒë·∫•t hi·ªán h√†nh, ƒë·∫•t ·ªü h·∫ªm s√¢u ·ªü ƒë∆∞·ªùng Nguy·ªÖn Xi·ªÉn c√≥ gi√° 28 tri·ªáu ƒë·ªìng m·ªói m2, trong khi ƒë·∫•t n√¥ng nghi·ªáp ch·ªâ kho·∫£ng 600.000 ƒë·ªìng, ch√™nh h∆°n 27 tri·ªáu ƒë·ªìng m·ªói m2, khi·∫øn chi ph√≠ chuy·ªÉn ƒë·ªïi tƒÉng tr√™n 10 l·∫ßn. Tr∆∞·ªõc ƒë√¢y, gi√° ƒë·∫•t ·ªü khu v·ª±c n√†y ch·ªâ 2,7 tri·ªáu ƒë·ªìng m·ªói m2.\",\n",
    "    \"Kh√¥ng ri√™ng ch·ªã H·∫°nh, nhi·ªÅu ng∆∞·ªùi d√¢n ph·∫£i rao b√°n nh√† ƒë·∫•t gi√° r·∫ª do kh√¥ng kham n·ªïi nghƒ©a v·ª• t√†i ch√≠nh khi chuy·ªÉn m·ª•c ƒë√≠ch. √îng L√™ VƒÉn Quy·∫øt (ph∆∞·ªùng Long B√¨nh) c√≥ th·ª≠a ƒë·∫•t 210 m2 ƒë·ªß ƒëi·ªÅu ki·ªán chuy·ªÉn th·ªï c∆∞ t·∫°i ph∆∞·ªùng Long B√¨nh, TP HCM nh∆∞ng v∆∞·ªõng quy ho·∫°ch. Khi ƒë∆∞·ª£c g·ª° v∆∞·ªõng, √¥ng Quy·∫øt t√¨m hi·ªÉu th·ªß t·ª•c t√°ch th·ª≠a v√† chuy·ªÉn m·ª•c ƒë√≠ch th√¨ r·∫•t s·ªëc v√¨ ph·∫£i ƒë√≥ng h∆°n 5,7 t·ª∑ ƒë·ªìng ti·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, v∆∞·ª£t kh·∫£ nƒÉng chi tr·∫£.\",\n",
    "    \"Tr∆∞·ªùng ƒë·∫°i h·ªçc b√°ch khoa h√† n·ªôi l√† m·ªôt trong nh·ªØng tr∆∞·ªùng h√†ng ƒë·∫ßu\"\n",
    "]\n",
    "\n",
    "print(\"=== Vietnamese Word Segmentation Demo ===\\n\")\n",
    "\n",
    "for i, text in enumerate(examples, 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    # Basic segmentation\n",
    "    segmented = segmenter.segment(text)\n",
    "    print(f\"Segmented: {segmented}\")\n",
    "    \n",
    "    # Get word boundaries (optional)\n",
    "    boundaries = segmenter.get_word_boundaries(text)\n",
    "    print(f\"Word boundaries: {len(boundaries)} words found\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Batch processing example\n",
    "print(\"\\n=== Batch Processing ===\")\n",
    "batch_results = segmenter.segment_batch(examples)\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"Batch {i}: {result}\")\n",
    "\n",
    "# Using the class as a callable\n",
    "print(\"\\n=== Using as Callable ===\")\n",
    "callable_result = segmenter(\"T√¥i y√™u Vi·ªát Nam\")\n",
    "print(f\"Callable result: {callable_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d89c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838753c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
