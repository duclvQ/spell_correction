{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "pipeline = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"vinai/phobert-base\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "def remove_special_characters(text):\n",
    "    import re\n",
    "    # Remove special characters and digits, keep only Vietnamese characters and spaces\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ỹ\\s]', '', text)\n",
    "    return text.strip()\n",
    "def topk_predictions(text, top_k=5):\n",
    "    # input_ids = tokenizer([text], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # logits = bartpho(input_ids).logits\n",
    "    # masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "    # probs = logits[0, masked_index].softmax(dim=0)\n",
    "    # values, predictions = probs.topk(top_k)\n",
    "    \n",
    "    # return tokenizer.decode(predictions).split()\n",
    "    \n",
    "    output = pipeline(text, top_k=top_k)\n",
    "    mask_index = text.index('<mask>')\n",
    "    # Extract the predictions from the output\n",
    "    predictions = []\n",
    "    for item in output:\n",
    "        if 'token_str' in item:\n",
    "            predictions.append(item['token_str'])\n",
    "    \n",
    "    predictions = [remove_special_characters(item['token_str']) for item in output]\n",
    "\n",
    "def sentence_prediction(text, top_k=300):\n",
    "    error_indices = []\n",
    "    for i in range(len(text.split())):\n",
    "        original_word\n",
    "        new_list = text.split()\n",
    "        # replace the i-th word with a mask token\n",
    "        # clean_word = remove_special_characters(new_list[i])\n",
    "        clean_word = new_list[i]\n",
    "        new_list[i] = \"<mask>\"\n",
    "        masked_text = \" \".join(new_list)\n",
    "        start_time = time.time()\n",
    "        predictions = topk_predictions(masked_text, top_k=top_k)\n",
    "        # for j in range(len(predictions)):\n",
    "        #     predictions[j] = remove_special_characters(predictions[j])\n",
    "        end_time = time.time()\n",
    "        if clean_word not in predictions:\n",
    "            error_indices.append((i, clean_word, predictions[0]))\n",
    "            print(f\"Masked Text: {masked_text}\")\n",
    "            print(f\"->Top {top_k} Predictions: {predictions}\")\n",
    "            \n",
    "        # print(f\"Masked Text: {masked_text}\")\n",
    "    return error_indices\n",
    "\n",
    "def paragraph_prediction(text, top_k=300):\n",
    "    sentences = text.split('. ')\n",
    "    all_errors = []\n",
    "    for sentence in sentences:\n",
    "        errors = sentence_prediction(sentence, top_k=top_k)\n",
    "        if errors:\n",
    "            all_errors.append((sentence, errors))\n",
    "            \n",
    "    return all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1dc03db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Text: Theo thầy Đỗ Đình <mask> Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường 20/8 tới đây.\n",
      "->Top 100 Predictions: ['Th', 'T', 'C', 'Hùng', 'Ph', 'Ho', 'Trung', 'Phụ', 'Kh', 'Qu', 'Tuấn', 'L', 'Đức', 'Vinh', 'Khánh', 'Tr', 'Thuận', 'Hồng', 'H', 'Tiến', 'Việt', 'Sơn', 'Hà', 'Giao', 'Thắng', 'Thu', 'Ngh', 'Khoa', 'Hiệp', 'Khả', 'Bình', 'Diễn', 'Tu', 'Ki', 'Châu', 'Quý', 'S', 'Tài', 'Hu', 'Phi', 'Bá', 'D', 'Phong', 'Viên', 'Đ', 'Ng', 'Tân', 'Tâm', 'Hợp', 'Ngọc', 'Ch', 'Phúc', 'Thái', 'Á', 'Hưng', 'Phú', 'Thanh', 'Dũng', 'Â', 'Duy', 'Toàn', 'Quy', 'Liên', 'Dung', 'Phương', 'Long', 'Thảo', 'Nguyên', 'Chương', 'Hoàn', 'X', 'Trọng', 'Thị', 'M', 'Bí', 'Thi', 'Di', 'Hi', 'Thành', 'Độ', 'Minh', 'Tho', 'Loan', 'Văn', 'Tru', 'Quang', 'Hiển', 'V', 'Thân', 'Lợi', 'Bảo', 'B', 'Tuy', 'Anh', 'Tha', 'K', 'San', 'Hải', 'Chính']\n",
      "Masked Text: Theo thầy Đỗ Đình Đảo, Hiệu <mask> những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường 20/8 tới đây.\n",
      "->Top 100 Predictions: ['trưởng', 'phó', 'trường', 'Trưởng', 'Phó', 'Trường', 'nhà', 'bộ', 'Hiệu', 'tru', 'tr', 'trương', 'phụ', 'giáo', 'đội', 'trươ', 'học', 'chính', 'Đại', 'quản', 'ban', 'chủ', 'hiệu', 'Học', 'giảngTr', 'các', 'M', 'sinh', '-', 'Văn', 'Trương', 'chán', 'đại', 'người', 'sát', 'Quản', 'Ban', 'giám', 'Chủ', 'u', 'S', '\"', 'đề', 'thầy', 'm', 'từ', 'ngành,', 'sách', 'Thu', 'quả', 'Hoa', 'thuốc', 'Phòng', 'đính', 'nội', 'cha', 'thư', 'G', 'Giáo', 'nói', 'thiết', 'chuẩn', 'Giám', 'viên', 'cán', 'To', 'Qu', 'Th', 'Chánh', 'huynh', 'hướng', 'tường', 'Bộ', 'Cao', 'Nhà', 'Y', 'x', 'viện', 'nhiệm', 'tài', 'Minh', 'L', 'toán', 'Lý', 'văn', 'mẫu', 'Chính', 'Độ', 'thu', 'tưởng', 'Ho', '/', 'Huy', 's', 'P', 'cao', '(']\n",
      "Masked Text: Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh <mask> ngày tựu trường 20/8 tới đây.\n",
      "->Top 100 Predictions: ['và', 'vào,', 'ngay', 'trong', 'toàn', 'trước', 'tại', 'biết', 'lớp', 'trên', 'của', 'từ', 'cả', 'thông', 'học', 'theo', 'về', 'qua', 'bằng', 'kịp', 'cũng', 'ở', 'sau', '(', 'trường', 'ngày', 'khi', 'đầu', 'một', 'cụ', 'nhà', 'cùng', 'các', 'được', 'chính', 'đến', 'hôm', 'đúng', 'chậm', 'dự', 'tham', 'đầy', 'tự', 'để', 'nội', 'sớm', 'rõ', 'cuối', 'đăng', 'Trường', 'có', '-', 'chủ', 'nhân', 'xem', 'nắm', 'nghe', 'kể', 'khối', 'nhận', 'ghi', 'THPT', 'thuộc', 'hoặc', '/', 'bắt', 'làm', 'trực', 'không', 'HS', 'phổ', 'đồng', 'là', 'chỉ', 'sinh', 'muộn', 'tập', 'thay', 'công', 'ít', 'mới', 'rất', 'trung', 'ký', 'phụ', 'nhanh', 'tổ', 'tới', 'sẽ', 'địa.', 'những', 'nơi', 'hai', 'chi', 'tất', 'hoàn', 'từng', 'hàng']\n",
      "Masked Text: Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày <mask> trường 20/8 tới đây.\n",
      "->Top 100 Predictions: ['khai', 'tự', 'lễ', 'kỷ', 'Nhà', 'bế', 'hội', 'tổng', 'họp', 'Lễ', 'nghỉ', 'đầu', 'nhà', 'ra', 'Phụ', 'phụ', 'sinh', '20', 'Kha', 'tạm', 'phát', 'Hội', 'chính', 'học', 'Tự', 'ch', 'toàn', 'gi', 'truyền', 'làm', 'thứ', 'mở', 'cuối', 'về', 'vui', 'k', 'Quốc', 'thành', 'sơ', 'nhập', 'Ngày', 'chia', 'tuyên', 'khá', 'Hi', 'Tổng', 'đưa', 'tổ', 'trở', '\"', 'bắt', 'tan', 'r', 'Chủ', 'đặc', 'biểu', 'trọng', 'chào', 'gặp', '30', 'chủ', '19', 'nhận', 'diễn', 'dự', 'hiệu', 'tốt', 'K', 'Tết', 'chuyển', 'di', 'hôm', 'tất', 'gửi', '15', 'T', 'Bảo', 'G', 'Gi', 'Tất', 'xả', 'Hiệu', '25', 'hiến', 'quay', 'Trung', 'đón', 'Phát', 'kết', '18', 'rời', 'bảo', 'thi', 'ngày', 'xuất', 'điều', 'đi', 'thường', 'kiểm']\n",
      "Masked Text: Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường <mask> tới đây.\n",
      "->Top 100 Predictions: ['20,', '19', '10', '22', '15', '21', '9', 'ngày', '27', '25', 'vào', '(', '17', '26', '1', '5', '3', '23', '2', '29', '8', '28', '7', '18', '30', '4', '14', '11', '12', '(20', '6', '13', '20.', '31', 'của', '24', 'và', '-', 'chính', '(1', 'sắp', '16', '(10', 'đầu', 'tháng', '(2', '(19', '10.', 'thứ', '(5', 'hôm', '(27', '(22', '(25', '(12', '(15', 'học', '26.', '22.', '(9', '19.', 'cuối', '(21', '1/8', '15.', '(23', '(17', '27.', '(26', 'dự', '(11', 'là', '(28.', '21.', '29.', 'diễn', '(6', '(24', '1/3', '17.', '09', 'năm', '28.', '18.', '23.', '25.', '(30', '11.', '12.', '01', '(18', '1.', '(3', '14.', '(29', '(4', '1.9']\n",
      "Masked Text: Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường 20/8 <mask> đây.\n",
      "->Top 100 Predictions: ['tới./2010,', 'và', 'sắp/2011/2009', 'này', 'vừa', 'năm/2012', 'tại/2017', '-', 'như/2018', 'hoặc', 'của/2016/2013', 'nhưng', 'trên', '(', 'theo/2015', 'mới/2014', 'cũng', 'thay', 'trước', 'sau', 'chứ', 'cùng', 'để', 'trong', 'gần', 'âm', 'với', 'đến/', 'học', 'tức/2008;/2007', 'thông', 'ở', 'hay', 'đúng', '/-2010', 'dưới', 'nhằm', 'nên/2006', 'vì', 'khi', 'hàng', 'so', 'vào', 'sẽ', 'chính', 'không', 'mà', 'cho', 'đồng', 'một', 'được', 'rồi', 'dự', 'sang', 'từ:', 'diễn...', 'tổ', 'giống', 'ngay', 'dương', 'song', 'tương', 'do', 'dành/2005-2011', 'bằng', 'nói', 'nếu', 'qua', '\"', 'tiếp', 'sớm-2009', 'có', 'giờ', 'cụ', 'trở', '2011', 'đây']\n",
      "Masked Text: Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường 20/8 tới <mask>\n",
      "->Top 100 Predictions: ['.', 'đây,', 'và', '(', 'tại...', 'để', 'trên', 'của', 'nhưng', 'theo', 'hoặc', '-!', 'cùng', 'trong', 'ở', 'này', 'cũng;', 'chứ', 'khi', 'sau', 'nay', 'với:', 'học', '(16', 'gần', 'do', 'nhằm', 'mà', 'trước', 'như', '(3', 'nên', '(11', 'là', '(5', '(15', '(4', '(28', 'Đây', 'đúng', '(10', 'thay', 'vào', '(17', 'đó', 'dưới', 'vì', 'nếu', '(12', 'ngày', '(1', '(24', 'thông', '(9', '(14', '(6', '(13', '(30', 'ngay', 'cho', 'rồi', 'sẽ', '(7', 'tới', '(8', 'sắp', '(22', 'hay', 'thời', 'vừa', 'các', 'được', 'thôi', '(29', '(31', '\"', 'qua', 'dự', '(25', '(18', 'làm', '(27', '(21', '(26', '(2', 'có', '(20', '(19', 'hết', '(23', 'trường', 'nhân', 'tận', 'đầy', 'đồng']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường 20/8 tới đây.',\n",
       "  [(4, 'Đảo,', 'Th'),\n",
       "   (6, 'trưởng,', 'trưởng'),\n",
       "   (18, 'vào', 'và'),\n",
       "   (20, 'tựu', 'khai'),\n",
       "   (22, '20/8', '20,'),\n",
       "   (23, 'tới', 'tới./2010,'),\n",
       "   (24, 'đây.', '.')])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TXT = \"Trung tâm Dự báo khí tượng thủy văn quốc gia cho biết lúc 7h hôm nay, áp thấp nhiệt đới mạnh 61 km/h, cấp 6-7, \"\n",
    "TXT = \"Theo thầy Đỗ Đình Đảo, Hiệu trưởng, những điều chỉnh này sẽ được thông báo cho học sinh vào ngày tựu trường 20/8 tới đây.\"\n",
    "paragraph_prediction(TXT, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "887b7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BARTpho-word\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n",
    "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2dfc5eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqModelOutput' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m TXT = \u001b[33m'\u001b[39m\u001b[33mTrung tâm Dự báo khí tượng thủy văn quốc gia cho biết lúc 7h hôm nay, áp thấp nhiệt đới mạnh 61 km/h, cấp 6-7,\u001b[39m\u001b[33m'\u001b[39m  \n\u001b[32m      2\u001b[39m input_ids = word_tokenizer(TXT, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m features = bartpho_word(input_ids).logits\n",
      "\u001b[31mAttributeError\u001b[39m: 'Seq2SeqModelOutput' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "TXT = 'Trung tâm Dự báo khí tượng thủy văn quốc gia cho biết lúc 7h hôm nay, áp thấp nhiệt đới mạnh 61 km/h, cấp 6-7,'  \n",
    "input_ids = word_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_word(input_ids).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e1dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     features = phobert(input_ids)  \u001b[38;5;66;03m# Models outputs are now tuples\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Extract the logits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m logits = features.logits\n",
      "\u001b[31mAttributeError\u001b[39m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Tôi là <mask> trường đại_học Công_nghệ .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed1b381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples\n",
    "\n",
    "# Extract the logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.51025390625,\n",
       "  'token': 3716,\n",
       "  'token_str': 'Hiệu_trưởng',\n",
       "  'sequence': 'Tôi là Hiệu_trưởng trường đại_học Công_nghệ .'},\n",
       " {'score': 0.2998046875,\n",
       "  'token': 4856,\n",
       "  'token_str': 'hiệu_trưởng',\n",
       "  'sequence': 'Tôi là hiệu_trưởng trường đại_học Công_nghệ .'},\n",
       " {'score': 0.039947509765625,\n",
       "  'token': 3466,\n",
       "  'token_str': 'giảng_viên',\n",
       "  'sequence': 'Tôi là giảng_viên trường đại_học Công_nghệ .'},\n",
       " {'score': 0.03753662109375,\n",
       "  'token': 250,\n",
       "  'token_str': 'Chủ_tịch',\n",
       "  'sequence': 'Tôi là Chủ_tịch trường đại_học Công_nghệ .'},\n",
       " {'score': 0.0199432373046875,\n",
       "  'token': 649,\n",
       "  'token_str': 'sinh_viên',\n",
       "  'sequence': 'Tôi là sinh_viên trường đại_học Công_nghệ .'},\n",
       " {'score': 0.00891876220703125,\n",
       "  'token': 1623,\n",
       "  'token_str': 'chủ_tịch',\n",
       "  'sequence': 'Tôi là chủ_tịch trường đại_học Công_nghệ .'},\n",
       " {'score': 0.0084381103515625,\n",
       "  'token': 693,\n",
       "  'token_str': 'Giám_đốc',\n",
       "  'sequence': 'Tôi là Giám_đốc trường đại_học Công_nghệ .'},\n",
       " {'score': 0.007450103759765625,\n",
       "  'token': 3332,\n",
       "  'token_str': 'giáo_sư',\n",
       "  'sequence': 'Tôi là giáo_sư trường đại_học Công_nghệ .'},\n",
       " {'score': 0.004245758056640625,\n",
       "  'token': 30929,\n",
       "  'token_str': 'cựu_học_sinh',\n",
       "  'sequence': 'Tôi là cựu_học_sinh trường đại_học Công_nghệ .'},\n",
       " {'score': 0.003688812255859375,\n",
       "  'token': 33952,\n",
       "  'token_str': 'Hiệu_phó',\n",
       "  'sequence': 'Tôi là Hiệu_phó trường đại_học Công_nghệ .'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12b55796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trường đại_học bách_khoa hà_nội'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "ViTokenizer.tokenize(u\"Trường đại học bách khoa hà nội\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ca4ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace uỷ with ủy\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT = \"Trung tâm Dự báo Khí tượng Thủy văn quốc gia cho biết lúc 7h hôm na, áp thấp nhiệt đới mạnh 61 km/h, cấp 6-7,\"\n",
    "# replace uỷ with ủy\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "\n",
    "ViTokenizer.tokenize(u\"Trường đại học bách khoa hà nội\")\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "def process_text(text):\n",
    "    for key, value in dict_map.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def sentence_prediction(text, top_k=30):\n",
    "    text = ViTokenizer.tokenize(TXT)    \n",
    "    \n",
    "    error_indices = []\n",
    "    for i in range(len(text.split())):\n",
    "        text = process_text(text)\n",
    "        new_list = text.split()\n",
    "        original_word = new_list[i]\n",
    "        if is_number(original_word):\n",
    "            continue\n",
    "        \n",
    "        new_list[i] = tokenizer.mask_token\n",
    "        masked_text = \" \".join(new_list)\n",
    "        start_time = time.time()\n",
    "        predictions = pipeline(masked_text, top_k=100)\n",
    "        topk = []\n",
    "        for item in predictions:\n",
    "            if 'token_str' in item:\n",
    "                topk.append(item['token_str'])\n",
    "        end_time = time.time()\n",
    "        if original_word not in topk:\n",
    "            print(f\"Original Word: {original_word}\")\n",
    "            print(f\"Masked Text: {masked_text}\")\n",
    "            print(f\"->Top 10 Predictions: {topk}\")\n",
    "            error_indices.append((i, original_word, topk[0]))\n",
    "        # print(f\"Masked Text: {masked_text}\")\n",
    "    return error_indices\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fadc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trước đy , giá đất ở khu_vực này chỉ 2 , 7 triệu đồng mỗi m2 .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "example = \"\"\"\n",
    "Trước đy, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.\n",
    "\"\"\"\n",
    "ner_results = nlp(example)\n",
    "example_tok = \"\"\n",
    "for e in ner_results:\n",
    "    if \"##\" in e[\"word\"]:\n",
    "        example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "    elif e[\"entity\"] ==\"I\":\n",
    "        example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "    else:\n",
    "        example_tok = example_tok + \" \" + e[\"word\"]\n",
    "print(example_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe6685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Trước đy, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.\n",
      "Segmented: Trước đy , giá đất ở khu_vực này chỉ 2 , 7 triệu đồng mỗi m2 .\n",
      "\n",
      "Sentence 1:\n",
      "Original: Trường đại học bách khoa hà nội\n",
      "Segmented: Trường đại_học bách_khoa hà_nội\n",
      "\n",
      "Sentence 2:\n",
      "Original: Thủ tướng Chính phủ Phạm Minh Chính\n",
      "Segmented: Thủ_tướng Chính_phủ Phạm_Minh_Chính\n",
      "\n",
      "Sentence 3:\n",
      "Original: Ngân hàng nhà nước Việt Nam\n",
      "Segmented: Ngân_hàng nhà_nước Việt_Nam\n"
     ]
    }
   ],
   "source": [
    "class VietnameseWordSegmenter:\n",
    "    \"\"\"\n",
    "    A Vietnamese word segmentation class using NlpHUST/vi-word-segmentation model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"NlpHUST/vi-word-segmentation\"):\n",
    "        \"\"\"\n",
    "        Initialize the Vietnamese word segmenter\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The pre-trained model name from HuggingFace\n",
    "        \"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.nlp = pipeline(\"token-classification\", model=self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "    def segment_text(self, text):\n",
    "        \"\"\"\n",
    "        Segment Vietnamese text into words\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input Vietnamese text to be segmented\n",
    "            \n",
    "        Returns:\n",
    "            str: Segmented text with underscores connecting compound words\n",
    "        \"\"\"\n",
    "        # Clean the input text\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Get NER results\n",
    "        ner_results = self.nlp(text)\n",
    "        \n",
    "        # Process the results to create segmented text\n",
    "        segmented_text = \"\"\n",
    "        for entity in ner_results:\n",
    "            word = entity[\"word\"]\n",
    "            entity_label = entity[\"entity\"]\n",
    "            \n",
    "            if \"##\" in word:\n",
    "                # Handle subword tokens (BERT tokenization artifacts)\n",
    "                segmented_text += word.replace(\"##\", \"\")\n",
    "            elif entity_label == \"I\":\n",
    "                # Inside entity - connect with underscore\n",
    "                segmented_text += \"_\" + word\n",
    "            else:\n",
    "                # Beginning of entity or outside entity - add space\n",
    "                segmented_text += \" \" + word\n",
    "                \n",
    "        return segmented_text.strip()\n",
    "    \n",
    "    def segment_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Segment multiple sentences\n",
    "        \n",
    "        Args:\n",
    "            sentences (list): List of Vietnamese sentences\n",
    "            \n",
    "        Returns:\n",
    "            list: List of segmented sentences\n",
    "        \"\"\"\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "            \n",
    "        segmented_sentences = []\n",
    "        for sentence in sentences:\n",
    "            segmented = self.segment_text(sentence)\n",
    "            segmented_sentences.append(segmented)\n",
    "            \n",
    "        return segmented_sentences\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Allow the class to be called directly\n",
    "        \n",
    "        Args:\n",
    "            text (str or list): Input text(s) to segment\n",
    "            \n",
    "        Returns:\n",
    "            str or list: Segmented text(s)\n",
    "        \"\"\"\n",
    "        if isinstance(text, list):\n",
    "            return self.segment_sentences(text)\n",
    "        else:\n",
    "            return self.segment_text(text)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the segmenter\n",
    "    segmenter = VietnameseWordSegmenter()\n",
    "    \n",
    "    # Test with example text\n",
    "    example_text = \"Trước đy, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.\"\n",
    "    segmented_result = segmenter.segment_text(example_text)\n",
    "    print(f\"Original: {example_text}\")\n",
    "    print(f\"Segmented: {segmented_result}\")\n",
    "    \n",
    "    # Test with multiple sentences\n",
    "    sentences = [\n",
    "        \"Trường đại học bách khoa hà nội\",\n",
    "        \"Thủ tướng Chính phủ Phạm Minh Chính\",\n",
    "        \"Ngân hàng nhà nước Việt Nam\"\n",
    "    ]\n",
    "    \n",
    "    segmented_sentences = segmenter.segment_sentences(sentences)\n",
    "    for i, (original, segmented) in enumerate(zip(sentences, segmented_sentences)):\n",
    "        print(f\"\\nSentence {i+1}:\")\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"Segmented: {segmented}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58666c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese Word Segmenter API is ready!\n",
      "To start the server, run: run_server()\n",
      "Or run: app.run(host='0.0.0.0', port=5000, debug=True)\n"
     ]
    }
   ],
   "source": [
    "# Flask API Interface\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize the segmenter (global instance)\n",
    "segmenter = None\n",
    "\n",
    "def initialize_segmenter():\n",
    "    \"\"\"Initialize the segmenter if not already initialized\"\"\"\n",
    "    global segmenter\n",
    "    if segmenter is None:\n",
    "        print(\"Initializing Vietnamese Word Segmenter...\")\n",
    "        segmenter = VietnameseWordSegmenter()\n",
    "        print(\"Segmenter initialized successfully!\")\n",
    "\n",
    "@app.route('/api/segment', methods=['POST'])\n",
    "def segment_text_api():\n",
    "    \"\"\"\n",
    "    API endpoint to segment Vietnamese text\n",
    "    \n",
    "    Expected JSON payload:\n",
    "    {\n",
    "        \"text\": \"Your Vietnamese text here\"\n",
    "    }\n",
    "    \n",
    "    Returns:\n",
    "    {\n",
    "        \"original\": \"original text\",\n",
    "        \"segmented\": \"segmented text\",\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize segmenter if needed\n",
    "        initialize_segmenter()\n",
    "        \n",
    "        # Get JSON data from request\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'text' not in data:\n",
    "            return jsonify({\n",
    "                'error': 'No text provided. Please send JSON with \"text\" field.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        input_text = data['text'].strip()\n",
    "        \n",
    "        if not input_text:\n",
    "            return jsonify({\n",
    "                'error': 'Empty text provided.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        # Perform segmentation\n",
    "        segmented_text = segmenter.segment_text(input_text)\n",
    "        \n",
    "        # Return results\n",
    "        return jsonify({\n",
    "            'original': input_text,\n",
    "            'segmented': segmented_text,\n",
    "            'status': 'success'\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'An error occurred: {str(e)}',\n",
    "            'status': 'error'\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/segment/batch', methods=['POST'])\n",
    "def segment_batch_api():\n",
    "    \"\"\"\n",
    "    API endpoint to segment multiple Vietnamese sentences\n",
    "    \n",
    "    Expected JSON payload:\n",
    "    {\n",
    "        \"texts\": [\"sentence 1\", \"sentence 2\", ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        initialize_segmenter()\n",
    "        \n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'texts' not in data:\n",
    "            return jsonify({\n",
    "                'error': 'No texts provided. Please send JSON with \"texts\" field.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        input_texts = data['texts']\n",
    "        \n",
    "        if not isinstance(input_texts, list):\n",
    "            return jsonify({\n",
    "                'error': 'texts field must be a list of strings.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        # Perform batch segmentation\n",
    "        segmented_texts = segmenter.segment_sentences(input_texts)\n",
    "        \n",
    "        # Return results\n",
    "        return jsonify({\n",
    "            'original': input_texts,\n",
    "            'segmented': segmented_texts,\n",
    "            'status': 'success'\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'An error occurred: {str(e)}',\n",
    "            'status': 'error'\n",
    "        }), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'service': 'Vietnamese Word Segmenter API',\n",
    "        'segmenter_initialized': segmenter is not None\n",
    "    })\n",
    "\n",
    "@app.route('/')\n",
    "def web_interface():\n",
    "    \"\"\"Web interface for the segmenter\"\"\"\n",
    "    return render_template_string(WEB_TEMPLATE)\n",
    "\n",
    "# Web interface HTML template\n",
    "WEB_TEMPLATE = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Vietnamese Word Segmenter</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        .container {\n",
    "            background: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .input-group {\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        label {\n",
    "            display: block;\n",
    "            margin-bottom: 8px;\n",
    "            font-weight: bold;\n",
    "            color: #555;\n",
    "        }\n",
    "        textarea {\n",
    "            width: 100%;\n",
    "            padding: 12px;\n",
    "            border: 2px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "            font-size: 14px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            resize: vertical;\n",
    "            min-height: 150px;\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        textarea:focus {\n",
    "            border-color: #4CAF50;\n",
    "            outline: none;\n",
    "        }\n",
    "        .button-group {\n",
    "            text-align: center;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        button {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "            padding: 12px 30px;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            cursor: pointer;\n",
    "            font-size: 16px;\n",
    "            margin: 0 10px;\n",
    "        }\n",
    "        button:hover {\n",
    "            background-color: #45a049;\n",
    "        }\n",
    "        button:disabled {\n",
    "            background-color: #ccc;\n",
    "            cursor: not-allowed;\n",
    "        }\n",
    "        .clear-btn {\n",
    "            background-color: #f44336;\n",
    "        }\n",
    "        .clear-btn:hover {\n",
    "            background-color: #da190b;\n",
    "        }\n",
    "        .output-area {\n",
    "            background-color: #f8f8f8;\n",
    "            border: 2px solid #eee;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .status {\n",
    "            text-align: center;\n",
    "            padding: 10px;\n",
    "            margin: 10px 0;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .status.success {\n",
    "            background-color: #d4edda;\n",
    "            color: #155724;\n",
    "        }\n",
    "        .status.error {\n",
    "            background-color: #f8d7da;\n",
    "            color: #721c24;\n",
    "        }\n",
    "        .status.loading {\n",
    "            background-color: #cce7ff;\n",
    "            color: #004085;\n",
    "        }\n",
    "        .example-texts {\n",
    "            background-color: #e7f3ff;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .example-text {\n",
    "            cursor: pointer;\n",
    "            color: #0066cc;\n",
    "            text-decoration: underline;\n",
    "            margin: 5px 0;\n",
    "            display: block;\n",
    "        }\n",
    "        .example-text:hover {\n",
    "            color: #004499;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>🇻🇳 Vietnamese Word Segmenter</h1>\n",
    "        \n",
    "        <div class=\"example-texts\">\n",
    "            <h3>📝 Example texts (click to use):</h3>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Trường đại học bách khoa hà nội')\">Trường đại học bách khoa hà nội</span>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Thủ tướng Chính phủ Phạm Minh Chính')\">Thủ tướng Chính phủ Phạm Minh Chính</span>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Ngân hàng nhà nước Việt Nam')\">Ngân hàng nhà nước Việt Nam</span>\n",
    "            <span class=\"example-text\" onclick=\"setInputText('Trước đây, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.')\">Trước đây, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.</span>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-group\">\n",
    "            <label for=\"inputText\">Original Text:</label>\n",
    "            <textarea id=\"inputText\" placeholder=\"Enter Vietnamese text here...\"></textarea>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"button-group\">\n",
    "            <button onclick=\"segmentText()\">🔍 Segment Text</button>\n",
    "            <button class=\"clear-btn\" onclick=\"clearText()\">🗑️ Clear All</button>\n",
    "        </div>\n",
    "        \n",
    "        <div id=\"status\"></div>\n",
    "        \n",
    "        <div class=\"input-group\">\n",
    "            <label for=\"outputText\">Segmented Text:</label>\n",
    "            <textarea id=\"outputText\" class=\"output-area\" readonly placeholder=\"Segmented text will appear here...\"></textarea>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        function setInputText(text) {\n",
    "            document.getElementById('inputText').value = text;\n",
    "        }\n",
    "        \n",
    "        function clearText() {\n",
    "            document.getElementById('inputText').value = '';\n",
    "            document.getElementById('outputText').value = '';\n",
    "            document.getElementById('status').innerHTML = '';\n",
    "        }\n",
    "        \n",
    "        function showStatus(message, type) {\n",
    "            const statusDiv = document.getElementById('status');\n",
    "            statusDiv.innerHTML = message;\n",
    "            statusDiv.className = `status ${type}`;\n",
    "        }\n",
    "        \n",
    "        async function segmentText() {\n",
    "            const inputText = document.getElementById('inputText').value.trim();\n",
    "            const outputText = document.getElementById('outputText');\n",
    "            \n",
    "            if (!inputText) {\n",
    "                showStatus('Please enter some text to segment.', 'error');\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Show loading status\n",
    "            showStatus('Processing text...', 'loading');\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch('/api/segment', {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify({\n",
    "                        text: inputText\n",
    "                    })\n",
    "                });\n",
    "                \n",
    "                const result = await response.json();\n",
    "                \n",
    "                if (result.status === 'success') {\n",
    "                    outputText.value = result.segmented;\n",
    "                    showStatus('✅ Text segmented successfully!', 'success');\n",
    "                } else {\n",
    "                    showStatus(`❌ Error: ${result.error}`, 'error');\n",
    "                    outputText.value = '';\n",
    "                }\n",
    "                \n",
    "            } catch (error) {\n",
    "                showStatus(`❌ Error: ${error.message}`, 'error');\n",
    "                outputText.value = '';\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Allow Enter key to trigger segmentation (with Shift+Enter for new line)\n",
    "        document.getElementById('inputText').addEventListener('keydown', function(event) {\n",
    "            if (event.key === 'Enter' && !event.shiftKey) {\n",
    "                event.preventDefault();\n",
    "                segmentText();\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Function to run the server\n",
    "def run_server(host='0.0.0.0', port=5000, debug=True):\n",
    "    \"\"\"\n",
    "    Run the Flask server\n",
    "    \n",
    "    Args:\n",
    "        host (str): Host address\n",
    "        port (int): Port number\n",
    "        debug (bool): Enable debug mode\n",
    "    \"\"\"\n",
    "    print(f\"Starting Vietnamese Word Segmenter API server...\")\n",
    "    print(f\"Web interface will be available at: http://localhost:{port}\")\n",
    "    print(f\"API endpoint: http://localhost:{port}/api/segment\")\n",
    "    \n",
    "    app.run(host=host, port=port, debug=debug)\n",
    "\n",
    "# Example of how to run the server\n",
    "print(\"Vietnamese Word Segmenter API is ready!\")\n",
    "print(\"To start the server, run: run_server()\")\n",
    "print(\"Or run: app.run(host='0.0.0.0', port=5000, debug=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server\n",
    "# Note: This will run the server and block the cell execution\n",
    "# The web interface will be available at http://localhost:5000\n",
    "# API endpoint will be available at http://localhost:5000/api/segment\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the server\n",
    "    run_server(host='0.0.0.0', port=5000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433b67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseWordSegmenter:\n",
    "    \"\"\"\n",
    "    A class for Vietnamese word segmentation using transformers.\n",
    "    \n",
    "    This class provides methods to segment Vietnamese text into words,\n",
    "    handling subword tokens and word boundaries properly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"NlpHUST/vi-word-segmentation\"):\n",
    "        \"\"\"\n",
    "        Initialize the Vietnamese Word Segmenter.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained model to use.\n",
    "                            Default is \"NlpHUST/vi-word-segmentation\"\n",
    "        \"\"\"\n",
    "        print(f\"Loading Vietnamese Word Segmentation model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.pipeline = pipeline(\"token-classification\", \n",
    "                                model=self.model, \n",
    "                                tokenizer=self.tokenizer)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def segment(self, text):\n",
    "        \"\"\"\n",
    "        Segment Vietnamese text into words.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The Vietnamese text to segment\n",
    "            \n",
    "        Returns:\n",
    "            str: The segmented text with words separated by spaces and \n",
    "                 compound words connected by underscores\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Get token classification results\n",
    "        ner_results = self.pipeline(text.strip())\n",
    "        \n",
    "        # Process the results to create segmented text\n",
    "        segmented_text = self._process_tokens(ner_results)\n",
    "        \n",
    "        return segmented_text.strip()\n",
    "    \n",
    "    def _process_tokens(self, ner_results):\n",
    "        \"\"\"\n",
    "        Process the token classification results to create segmented text.\n",
    "        \n",
    "        Args:\n",
    "            ner_results (list): List of token classification results\n",
    "            \n",
    "        Returns:\n",
    "            str: Processed segmented text\n",
    "        \"\"\"\n",
    "        segmented_text = \"\"\n",
    "        \n",
    "        for token_info in ner_results:\n",
    "            word = token_info[\"word\"]\n",
    "            entity = token_info[\"entity\"]\n",
    "            \n",
    "            if \"##\" in word:\n",
    "                # Handle subword tokens (remove ## prefix)\n",
    "                segmented_text += word.replace(\"##\", \"\")\n",
    "            elif entity == \"I\":\n",
    "                # Inside entity - connect with underscore\n",
    "                segmented_text += \"_\" + word\n",
    "            else:\n",
    "                # Beginning of entity or outside entity - add space\n",
    "                segmented_text += \" \" + word\n",
    "        \n",
    "        return segmented_text\n",
    "    \n",
    "    def segment_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Segment multiple texts at once.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of Vietnamese texts to segment\n",
    "            \n",
    "        Returns:\n",
    "            list: List of segmented texts\n",
    "        \"\"\"\n",
    "        return [self.segment(text) for text in texts]\n",
    "    \n",
    "    def get_word_boundaries(self, text):\n",
    "        \"\"\"\n",
    "        Get detailed word boundary information.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The Vietnamese text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            list: List of dictionaries containing word information\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        \n",
    "        ner_results = self.pipeline(text.strip())\n",
    "        word_info = []\n",
    "        current_word = \"\"\n",
    "        current_start = 0\n",
    "        \n",
    "        for i, token_info in enumerate(ner_results):\n",
    "            word = token_info[\"word\"]\n",
    "            entity = token_info[\"entity\"]\n",
    "            start = token_info.get(\"start\", 0)\n",
    "            end = token_info.get(\"end\", 0)\n",
    "            \n",
    "            if \"##\" in word:\n",
    "                current_word += word.replace(\"##\", \"\")\n",
    "            elif entity == \"I\":\n",
    "                current_word += \"_\" + word\n",
    "            else:\n",
    "                # Save previous word if exists\n",
    "                if current_word:\n",
    "                    word_info.append({\n",
    "                        \"word\": current_word.strip(),\n",
    "                        \"start\": current_start,\n",
    "                        \"end\": end\n",
    "                    })\n",
    "                \n",
    "                # Start new word\n",
    "                current_word = word\n",
    "                current_start = start\n",
    "        \n",
    "        # Add the last word\n",
    "        if current_word:\n",
    "            word_info.append({\n",
    "                \"word\": current_word.strip(),\n",
    "                \"start\": current_start,\n",
    "                \"end\": ner_results[-1].get(\"end\", len(text)) if ner_results else len(text)\n",
    "            })\n",
    "        \n",
    "        return word_info\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Make the class callable - same as segment method.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The Vietnamese text to segment\n",
    "            \n",
    "        Returns:\n",
    "            str: The segmented text\n",
    "        \"\"\"\n",
    "        return self.segment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13219db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vietnamese Word Segmentation model: NlpHUST/vi-word-segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "=== Vietnamese Word Segmentation Demo ===\n",
      "\n",
      "Example 1:\n",
      "Original: Theo bảng giá đất hiện hành, đất ở hẻm sâu ở đường Nguyễn Xiển có giá 28 triệu đồng mỗi m2, trong khi đất nông nghiệp chỉ khoảng 600.000 đồng, chênh hơn 27 triệu đồng mỗi m2, khiến chi phí chuyển đổi tăng trên 10 lần. Trước đây, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.\n",
      "Segmented: Theo bảng giá đất hiện_hành , đất ở hẻm sâu ở đường Nguyễn_Xiển có_giá 28 triệu đồng mỗi m2 , trong khi đất nông_nghiệp chỉ khoảng 600 . 000 đồng , chênh hơn 27 triệu đồng mỗi m2 , khiến chi_phí chuyển_đổi tăng trên 10 lần . Trước_đây , giá đất ở khu_vực này chỉ 2 , 7 triệu đồng mỗi m2 .\n",
      "Word boundaries: 63 words found\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Original: Không riêng chị Hạnh, nhiều người dân phải rao bán nhà đất giá rẻ do không kham nổi nghĩa vụ tài chính khi chuyển mục đích. Ông Lê Văn Quyết (phường Long Bình) có thửa đất 210 m2 đủ điều kiện chuyển thổ cư tại phường Long Bình, TP HCM nhưng vướng quy hoạch. Khi được gỡ vướng, ông Quyết tìm hiểu thủ tục tách thửa và chuyển mục đích thì rất sốc vì phải đóng hơn 5,7 tỷ đồng tiền sử dụng đất, vượt khả năng chi trả.\n",
      "Segmented: Không riêng chị Hạnh , nhiều người_dân phải rao bán nhà_đất giá rẻ do không kham nổi nghĩa_vụ tài_chính khi chuyển mục_đích . Ông Lê_Văn_Quyết ( phường Long_Bình ) có thửa đất 210 m2 đủ điều_kiện chuyển thổ_cư tại phường Long_Bình , TP HCM nhưng vướng quy_hoạch . Khi được gỡ vướng , ông Quyết tìm_hiểu thủ_tục tách thửa và chuyển mục_đích thì rất sốc vì phải đóng hơn 5 , 7 tỷ đồng tiền sử_dụng đất , vượt khả_năng chi_trả .\n",
      "Word boundaries: 82 words found\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Original: Trường đại học bách khoa hà nội là một trong những trường hàng đầu\n",
      "Segmented: Trường đại_học bách_khoa hà_nội là một trong những trường hàng_đầu\n",
      "Word boundaries: 10 words found\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Batch Processing ===\n",
      "Batch 1: Theo bảng giá đất hiện_hành , đất ở hẻm sâu ở đường Nguyễn_Xiển có_giá 28 triệu đồng mỗi m2 , trong khi đất nông_nghiệp chỉ khoảng 600 . 000 đồng , chênh hơn 27 triệu đồng mỗi m2 , khiến chi_phí chuyển_đổi tăng trên 10 lần . Trước_đây , giá đất ở khu_vực này chỉ 2 , 7 triệu đồng mỗi m2 .\n",
      "Batch 2: Không riêng chị Hạnh , nhiều người_dân phải rao bán nhà_đất giá rẻ do không kham nổi nghĩa_vụ tài_chính khi chuyển mục_đích . Ông Lê_Văn_Quyết ( phường Long_Bình ) có thửa đất 210 m2 đủ điều_kiện chuyển thổ_cư tại phường Long_Bình , TP HCM nhưng vướng quy_hoạch . Khi được gỡ vướng , ông Quyết tìm_hiểu thủ_tục tách thửa và chuyển mục_đích thì rất sốc vì phải đóng hơn 5 , 7 tỷ đồng tiền sử_dụng đất , vượt khả_năng chi_trả .\n",
      "Batch 3: Trường đại_học bách_khoa hà_nội là một trong những trường hàng_đầu\n",
      "\n",
      "=== Using as Callable ===\n",
      "Callable result: Tôi yêu Việt_Nam\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the VietnameseWordSegmenter class\n",
    "\n",
    "# Initialize the segmenter\n",
    "segmenter = VietnameseWordSegmenter()\n",
    "\n",
    "# Example texts to segment\n",
    "examples = [\n",
    "    \"Theo bảng giá đất hiện hành, đất ở hẻm sâu ở đường Nguyễn Xiển có giá 28 triệu đồng mỗi m2, trong khi đất nông nghiệp chỉ khoảng 600.000 đồng, chênh hơn 27 triệu đồng mỗi m2, khiến chi phí chuyển đổi tăng trên 10 lần. Trước đây, giá đất ở khu vực này chỉ 2,7 triệu đồng mỗi m2.\",\n",
    "    \"Không riêng chị Hạnh, nhiều người dân phải rao bán nhà đất giá rẻ do không kham nổi nghĩa vụ tài chính khi chuyển mục đích. Ông Lê Văn Quyết (phường Long Bình) có thửa đất 210 m2 đủ điều kiện chuyển thổ cư tại phường Long Bình, TP HCM nhưng vướng quy hoạch. Khi được gỡ vướng, ông Quyết tìm hiểu thủ tục tách thửa và chuyển mục đích thì rất sốc vì phải đóng hơn 5,7 tỷ đồng tiền sử dụng đất, vượt khả năng chi trả.\",\n",
    "    \"Trường đại học bách khoa hà nội là một trong những trường hàng đầu\"\n",
    "]\n",
    "\n",
    "print(\"=== Vietnamese Word Segmentation Demo ===\\n\")\n",
    "\n",
    "for i, text in enumerate(examples, 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    # Basic segmentation\n",
    "    segmented = segmenter.segment(text)\n",
    "    print(f\"Segmented: {segmented}\")\n",
    "    \n",
    "    # Get word boundaries (optional)\n",
    "    boundaries = segmenter.get_word_boundaries(text)\n",
    "    print(f\"Word boundaries: {len(boundaries)} words found\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Batch processing example\n",
    "print(\"\\n=== Batch Processing ===\")\n",
    "batch_results = segmenter.segment_batch(examples)\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"Batch {i}: {result}\")\n",
    "\n",
    "# Using the class as a callable\n",
    "print(\"\\n=== Using as Callable ===\")\n",
    "callable_result = segmenter(\"Tôi yêu Việt Nam\")\n",
    "print(f\"Callable result: {callable_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d89c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838753c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
